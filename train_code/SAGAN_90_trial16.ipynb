{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T16:33:06.551472Z",
     "start_time": "2019-08-13T16:33:03.794662Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# speed up the loading of the training data\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from onlineTripletloss import *\n",
    "from selector import *\n",
    "from model_SAGAN1_1 import NetG, NetD, NetA\n",
    "# from model_SAGAN2_Triplet import NetG, NetD, NetA\n",
    "# from model_WGANGP import NetG, NetD, NetA\n",
    "# from model_WGAN import NetG, NetD, NetA\n",
    "# from model_siGAN import NetG, NetD, NetA\n",
    "from dataset2Loader import CASIABDataset\n",
    "# from dataset2Loader_newtriplet import CASIABDataset\n",
    "# from dataset2Loader_triplet import CASIABDataset\n",
    "import torch.optim as optim\n",
    "import visdom\n",
    "from torchvision.utils import make_grid\n",
    "# Data_Dir = '../GaitRecognition/DatasetB_GEI_64x64_allseq/'\n",
    "Data_Dir = '../GaitRecognition/GEI_CASIA_B/gei/'\n",
    "Model_Name = 'Model_64x64_SAGAN_90_trial16'\n",
    "Model_dir = './Transform_Model/'+ Model_Name\n",
    "if not os.path.isdir(Model_dir):\n",
    "    os.mkdir(Model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:51:23.878764Z",
     "start_time": "2019-05-17T11:51:22.490886Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Data_Dir = '../GaitRecognition/GEI_CASIA_B/gei/'\n",
    "import os\n",
    "\n",
    "for r, d, files in os.walk(Data_Dir):\n",
    "    print(r)\n",
    "    print(len(d))\n",
    "    print(len(files))\n",
    "    \n",
    "# cpt = sum([len(files) for r, d, files in os.walk(Data_Dir)])\n",
    "# cpt = sum([len(d) for r, d, files in os.walk(Data_Dir)])\n",
    "# print(cpt)\n",
    "# list = os.listdir(Data_Dir) # dir is your directory path\n",
    "# number_files = len(list)\n",
    "# print(number_files)\n",
    "\n",
    "# import fnmatch\n",
    "# print(len(fnmatch.filter(os.listdir(Data_Dir), '*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T06:02:12.747164Z",
     "start_time": "2019-05-27T06:02:12.676816Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = CASIABDataset(data_dir=Data_Dir)\n",
    "ass_label, noass_label, img = dataset.getbatch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iteration 刪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T08:08:38.785029Z",
     "start_time": "2019-06-16T08:08:38.768214Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(tensor, gain=1.)\n",
    "#         init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir)\n",
    "\n",
    "iteration = 0\n",
    "lr = 0.00002\n",
    "# lr = 0.0002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 128\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=lr/2)\n",
    "optimD = optim.Adam(netd.parameters(), lr=lr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=lr/3)\n",
    "\n",
    "print('Training starts')\n",
    "while iteration < 30000:\n",
    "    ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "    ass_label = ass_label.to(device).to(th.float32)\n",
    "    noass_label = noass_label.to(device).to(th.float32)\n",
    "    img = img.to(device).to(th.float32)\n",
    "    \n",
    "    # update D\n",
    "    lossD = 0\n",
    "    optimD.zero_grad()\n",
    "    output = netd(ass_label)\n",
    "    label.fill_(real_label)\n",
    "    lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "    lossD += lossD_real1.item()\n",
    "    lossD_real1.backward()\n",
    "\n",
    "    label.fill_(real_label)\n",
    "    output1 = netd(noass_label)\n",
    "    lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "    lossD += lossD_real2.item()\n",
    "    lossD_real2.backward()\n",
    "\n",
    "    fake = netg(img).detach()\n",
    "    label.fill_(fake_label)\n",
    "    output2 = netd(fake)\n",
    "    lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "    lossD += lossD_fake.item()\n",
    "    lossD_fake.backward()\n",
    "\n",
    "    optimD.step()\n",
    "    \n",
    "    # update A\n",
    "    lossA = 0\n",
    "    optimA.zero_grad()\n",
    "    assd = th.cat((img, ass_label), 1)\n",
    "    noassd = th.cat((img, noass_label), 1)\n",
    "    fake = netg(img).detach()\n",
    "    faked = th.cat((img, fake), 1)\n",
    "\n",
    "    label.fill_(real_label)\n",
    "    output1 = neta(assd)\n",
    "    lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "    lossA += lossA_real1.item()\n",
    "    lossA_real1.backward()\n",
    "\n",
    "    label.fill_(fake_label)\n",
    "    output = neta(noassd)\n",
    "    lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "    lossA += lossA_real2.item()\n",
    "    lossA_real2.backward()\n",
    "\n",
    "    label.fill_(fake_label)\n",
    "    output = neta(faked)\n",
    "    lossA_fake = F.binary_cross_entropy(output, label)\n",
    "    lossA += lossA_fake.item()\n",
    "    lossA_fake.backward()\n",
    "    \n",
    "    optimA.step()\n",
    "    \n",
    "    # update G\n",
    "    lossG = 0\n",
    "    optimG.zero_grad()\n",
    "    fake = netg(img)\n",
    "    output = netd(fake)\n",
    "\n",
    "    label.fill_(real_label)\n",
    "    lossGD = F.binary_cross_entropy(output, label)\n",
    "    lossG += lossGD.item()\n",
    "    lossGD.backward(retain_graph=True)\n",
    "\n",
    "    faked = th.cat((img, fake), 1)\n",
    "    output = neta(faked)\n",
    "    label.fill_(real_label)\n",
    "    lossGA = F.binary_cross_entropy(output, label)\n",
    "    lossG += lossGA.item()\n",
    "    lossGA.backward()\n",
    "    \n",
    "    optimG.step()\n",
    "\n",
    "    iteration += 1\n",
    "    \n",
    "    if iteration % 20 == 0:\n",
    "        with th.no_grad():\n",
    "            netg.eval()  #切換\n",
    "            fake = netg(img) 25\n",
    "            netg.train() #切換回去\n",
    "        fake = (fake + 1) / 2 * 255\n",
    "        real = (ass_label + 1) / 2 * 255\n",
    "        ori = (img + 1) / 2 * 255\n",
    "        al = th.cat((fake, real, ori), 2)\n",
    "        display = make_grid(al, 20).cpu().numpy()\n",
    "        if win1 is None:\n",
    "            win1 = vis.image(display,\n",
    "                             opts=dict(title=\"train\", caption='train'))\n",
    "        else:\n",
    "            vis.image(display, win=win1)\n",
    "    if iteration % 1000 == 0 or iteration==500:\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_Name+'/snapshot'+ Model_Name +'_%d.t7' % iteration)\n",
    "\n",
    "    if iteration % 100==0 or iteration==10 :\n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[iteration, iteration,\n",
    "                                        iteration]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='iterations',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[iteration, iteration,\n",
    "                                  iteration]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "            \n",
    "        with open(Model_Name+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('iter = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            iteration, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('iter = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            iteration, lossG/2, lossA/3, lossD/3\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T05:25:59.661560Z",
     "start_time": "2019-05-27T05:25:59.620596Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = CASIABDataset(data_dir=Data_Dir)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=False)\n",
    "train_loader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T08:46:23.327475Z",
     "start_time": "2019-06-16T08:23:19.736325Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "\n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0\n",
    "margin = 0\n",
    "n_g = 0\n",
    "n_d = 0\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr/2)\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "print('Training starts')\n",
    "low_loss = 10\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#     for i, (ass_label, noass_label, noass_img, img, ass_img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "#         ass_img = ass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "#         noass_img = noass_img.to(device).to(th.float32)\n",
    "\n",
    "        if i % n_g ==0:\n",
    "            # update D\n",
    "            lossD = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "            label.fill_(real_label)\n",
    "            lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD += lossD_real1.item()\n",
    "            lossD_real1.backward()\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "            lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD += lossD_real2.item()\n",
    "            lossD_real2.backward()\n",
    "\n",
    "            fake = netg(img).detach()\n",
    "            label.fill_(fake_label)\n",
    "            output2 = netd(fake)\n",
    "            lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD += lossD_fake.item()\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            fake = netg(img).detach()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "            lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "            lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "            lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "\n",
    "        if i % n_d ==0: \n",
    "            # update G\n",
    "            lossG = 0\n",
    "            optimG.zero_grad()\n",
    "            fake = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGD.item()\n",
    "            lossGD.backward(retain_graph=True)\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "            label.fill_(real_label)\n",
    "            lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGA.item()\n",
    "            lossGA.backward()\n",
    "\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    \n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:  \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "    \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update k times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:08:32.658055Z",
     "start_time": "2019-06-15T08:18:03.792818Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0\n",
    "margin = 0\n",
    "n_g = 2\n",
    "n_d = 0\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr/2)\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#     for i, (ass_label, noass_label, noass_img, img, ass_img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "#         noass_img = noass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "#         ass_img = ass_img.to(device).to(th.float32)\n",
    "        \n",
    "        if i % n_g ==0:\n",
    "            # update D\n",
    "            lossD = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "            label.fill_(real_label)\n",
    "            lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD += lossD_real1.item()\n",
    "            lossD_real1.backward()\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "            lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD += lossD_real2.item()\n",
    "            lossD_real2.backward()\n",
    "\n",
    "            fake = netg(img).detach()\n",
    "            label.fill_(fake_label)\n",
    "            output2 = netd(fake)\n",
    "            lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD += lossD_fake.item()\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake = netg(img).detach()\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "            lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "            lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "            lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "        \n",
    "        if i % n_d ==0: \n",
    "            # update G\n",
    "            lossG = 0\n",
    "            optimG.zero_grad()\n",
    "            fake = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGD.item()\n",
    "            lossGD.backward(retain_graph=True)\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "            label.fill_(real_label)\n",
    "            lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGA.item()\n",
    "            lossGA.backward()\n",
    "\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    \n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model changed +Dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T09:39:59.973452Z",
     "start_time": "2019-06-17T01:57:16.530081Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netp = NetP(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netp.children())[0].children(),\n",
    "    list(neta.children())[0].children(),\n",
    "    list(netd.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "epoches = 700\n",
    "lr = 0.00002\n",
    "# lr = 0.0002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "\n",
    "netg = netg.to(device)\n",
    "netp = netp.to(device)\n",
    "neta = neta.to(device)\n",
    "netd = netd.to(device)\n",
    "netg.train()\n",
    "netp.train()\n",
    "neta.train()\n",
    "netd.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=lr/2)\n",
    "optimP = optim.Adam(netp.parameters(), lr=lr/3)\n",
    "optimD = optim.Adam(netd.parameters(), lr=lr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=lr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, lr = {}, batchsize = {}, target = {} \\n'.format(\n",
    "            epoches, lr, batchSize, target))\n",
    "\n",
    "print('Training starts')\n",
    "low_loss = 10\n",
    "for epoch in range(1,epoches+1):\n",
    "#     for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "    for i, (ass_label, noass_label, ass_img, img, noass_img) in enumerate(train_loader): \n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "    # while epoch < 30000:\n",
    "    #     ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        ass_img = ass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        noass_img = noass_img.to(device).to(th.float32)\n",
    "        \n",
    "        # update D\n",
    "        lossD = 0\n",
    "        optimD.zero_grad()\n",
    "        output = netd(ass_label)\n",
    "        label.fill_(real_label)\n",
    "        lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "        lossD += lossD_real1.item()\n",
    "        lossD_real1.backward()\n",
    "\n",
    "        label.fill_(real_label)\n",
    "        output1 = netd(noass_label)\n",
    "        lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "        lossD += lossD_real2.item()\n",
    "        lossD_real2.backward()\n",
    "\n",
    "        fake = netg(img).detach()\n",
    "        label.fill_(fake_label)\n",
    "        output2 = netd(fake)\n",
    "        lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "        lossD += lossD_fake.item()\n",
    "        lossD_fake.backward()\n",
    "\n",
    "        optimD.step()\n",
    "\n",
    "\n",
    "        # update P\n",
    "        lossP = 0\n",
    "        optimP.zero_grad()\n",
    "        fake = netg(img).detach()\n",
    "        assd = th.cat((img, ass_img), 1)\n",
    "        noassd = th.cat((img, noass_img), 1)\n",
    "        faked = th.cat((img, fake), 1)\n",
    "\n",
    "        label.fill_(real_label)\n",
    "        output1 = neta(assd)\n",
    "        lossP_real1 = F.binary_cross_entropy(output1, label)\n",
    "        lossP += lossP_real1.item()/2\n",
    "        lossP_real1 = lossP_real1/2\n",
    "        lossP_real1.backward()\n",
    "\n",
    "        label.fill_(fake_label)\n",
    "        output = neta(noassd)\n",
    "        lossP_real2 = F.binary_cross_entropy(output, label)\n",
    "        lossP += lossP_real2.item()/2\n",
    "        lossP_real2 = lossP_real2/2\n",
    "        lossP_real2.backward()\n",
    "\n",
    "        label.fill_(fake_label)\n",
    "        output = neta(faked)\n",
    "        lossP_fake = F.binary_cross_entropy(output, label)\n",
    "        lossP += lossP_fake.item()/2\n",
    "        lossP_fake = lossP_fake/2\n",
    "        lossP_fake.backward()\n",
    "\n",
    "        optimP.step()\n",
    "    \n",
    "\n",
    "        # update A\n",
    "        lossA = 0\n",
    "        optimA.zero_grad()\n",
    "        fake = netg(img).detach()\n",
    "        assd = th.cat((img, ass_label), 1)\n",
    "        noassd = th.cat((img, noass_label), 1)\n",
    "        faked = th.cat((img, fake), 1)\n",
    "\n",
    "        label.fill_(real_label)\n",
    "        output1 = neta(assd)\n",
    "        lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "        lossA += lossA_real1.item()\n",
    "        lossA_real1.backward()\n",
    "\n",
    "        label.fill_(fake_label)\n",
    "        output = neta(noassd)\n",
    "        lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "        lossA += lossA_real2.item()\n",
    "        lossA_real2.backward()\n",
    "\n",
    "        label.fill_(fake_label)\n",
    "        output = neta(faked)\n",
    "        lossA_fake = F.binary_cross_entropy(output, label)\n",
    "        lossA += lossA_fake.item()\n",
    "        lossA_fake.backward()\n",
    "\n",
    "        optimA.step()\n",
    "\n",
    "        # update G\n",
    "        lossG = 0\n",
    "        optimG.zero_grad()\n",
    "        \n",
    "        fake = netg(img)\n",
    "        output = netd(fake)\n",
    "        label.fill_(real_label)\n",
    "        lossGD = F.binary_cross_entropy(output, label)\n",
    "        lossG += lossGD.item()\n",
    "        lossGD.backward(retain_graph=True)\n",
    "\n",
    "        faked = th.cat((img, fake), 1)\n",
    "        output = neta(faked)\n",
    "        label.fill_(real_label)\n",
    "        lossGA = F.binary_cross_entropy(output, label)\n",
    "        lossG += lossGA.item()\n",
    "        lossGA.backward(retain_graph=True)\n",
    "        \n",
    "        faked = th.cat((img, fake), 1)\n",
    "        output = netp(faked)\n",
    "        label.fill_(real_label)\n",
    "        lossGP = F.binary_cross_entropy(output, label)\n",
    "        lossG += lossGP.item()/2\n",
    "        lossGP = lossGP/2\n",
    "        lossGP.backward()\n",
    "\n",
    "        optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    \n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch, epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3, lossP/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD', 'lossP']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch, epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3, lossP/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}, ErrP = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3, lossP/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}, ErrP = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3, lossP/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:  \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict(),\n",
    "            'netP': netp.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "    \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict(),\n",
    "            'netP': netp.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}, ErrP = {}  \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3, lossP/3\n",
    "        ))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model only D_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T05:21:59.261901Z",
     "start_time": "2019-06-23T02:19:01.856865Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "epoches = 700\n",
    "lr = 0.00002\n",
    "# lr = 0.0002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "g_k = 2\n",
    "target = '090'\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "# netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=lr/2)\n",
    "# optimD = optim.Adam(netd.parameters(), lr=lr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=lr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, lr = {}, batchsize = {}, g_k = {}, target={} \\n'.format(\n",
    "            epoches, lr, batchSize, g_k, target))\n",
    "\n",
    "low_loss = 10\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#     for i, (ass_label, noass_label, noass_img, img, ass_img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "    # while epoch < 30000:\n",
    "    #     ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "#         noass_img = noass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "#         ass_img = ass_img.to(device).to(th.float32)\n",
    "        \n",
    "        if i % g_k ==0:\n",
    "#             # update D\n",
    "#             lossD = 0\n",
    "#             optimD.zero_grad()\n",
    "#             output = netd(ass_label)\n",
    "#             label.fill_(real_label)\n",
    "#             lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "#             lossD += lossD_real1.item()\n",
    "#             lossD_real1.backward()\n",
    "\n",
    "#             label.fill_(real_label)\n",
    "#             output1 = netd(noass_label)\n",
    "#             lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "#             lossD += lossD_real2.item()\n",
    "#             lossD_real2.backward()\n",
    "\n",
    "#             fake = netg(img).detach()\n",
    "#             label.fill_(fake_label)\n",
    "#             output2 = netd(fake)\n",
    "#             lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "#             lossD += lossD_fake.item()\n",
    "#             lossD_fake.backward()\n",
    "\n",
    "#             optimD.step()\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake = netg(img).detach()\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "            lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "            lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "            lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "        \n",
    "#         if i % k ==0: \n",
    "        # update G\n",
    "        lossG = 0\n",
    "        optimG.zero_grad()\n",
    "        fake = netg(img)\n",
    "        output = netd(fake)\n",
    "\n",
    "        label.fill_(real_label)\n",
    "        lossGD = F.binary_cross_entropy(output, label)\n",
    "        lossG += lossGD.item()\n",
    "        lossGD.backward(retain_graph=True)\n",
    "\n",
    "        faked = th.cat((img, fake), 1)\n",
    "        output = neta(faked)\n",
    "        label.fill_(real_label)\n",
    "        lossGA = F.binary_cross_entropy(output, label)\n",
    "        lossG += lossGA.item()\n",
    "        lossGA.backward()\n",
    "\n",
    "        optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    \n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        ]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  ]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3,\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, \n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}\\n'.format(\n",
    "            epoch, lossG/2, lossA/3\n",
    "        ))    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaitGAN and triplet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T21:13:30.999315Z",
     "start_time": "2019-05-29T14:16:28.502269Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '036'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0\n",
    "margin = 5\n",
    "n_g = 0\n",
    "n_d = 0\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr/2)\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "\n",
    "        # update D\n",
    "        if i % n_g==0:\n",
    "            lossD = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "            label.fill_(real_label)\n",
    "            lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD += lossD_real1.item()\n",
    "            lossD_real1.backward()\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "            lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD += lossD_real2.item()\n",
    "            lossD_real2.backward()\n",
    "\n",
    "            fake, _ = netg(img)\n",
    "            label.fill_(fake_label)\n",
    "            output2 = netd(fake.detach())\n",
    "            lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD += lossD_fake.item()\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake,_ = netg(img)\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "            lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "            lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(faked.detach())\n",
    "            lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "        \n",
    "        if i % n_d ==0: \n",
    "            # update G\n",
    "            lossG = 0\n",
    "            optimG.zero_grad()\n",
    "            fake, A = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGD.item()\n",
    "            lossGD.backward(retain_graph=True)\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "            label.fill_(real_label)\n",
    "            lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGA.item()\n",
    "            lossGA.backward()\n",
    "\n",
    "            # constrain on generator\n",
    "            fake_ass, P = netg(ass_label)\n",
    "            fake_noass, N = netg(noass_label)\n",
    "            lossTriplet = F.triplet_margin_loss(fake, fake_ass, fake_noass, margin = margin)\n",
    "            lossG += lossTriplet.item()\n",
    "            lossTriplet.backward()\n",
    "\n",
    "            # constrain on encoder\n",
    "    #         __, P = netg(ass_label)\n",
    "    #         __, N = netg(noass_label)\n",
    "    #         lossTriplet = F.triplet_margin_loss(A, P, N, margin = margin)\n",
    "    #         lossG += lossTriplet.item()\n",
    "    #         lossTriplet.backward()\n",
    "\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaitGAN k times and triplet   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T17:27:36.724552Z",
     "start_time": "2019-06-18T06:30:54.000065Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0\n",
    "margin = 10\n",
    "n_g = 2\n",
    "n_d = 2\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr/3)\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, noass_img, img, ass_img) in enumerate(train_loader):\n",
    "#         com_img = th.cat((noass_img, img, ass_img), 0)\n",
    "#         com_label = th.cat(( label_neg, label_anc, label_pos), 0)\n",
    "#         com_img = com_img.to(device).to(th.float32)\n",
    "#         com_label = com_label.to(device).to(th.float32)\n",
    "        \n",
    "#         if(i ==0):\n",
    "#             print(label_neg,label_anc,label_pos)\n",
    "#             print(com_label)\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape,com_img.shape, com_label.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        noass_img = noass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        ass_img = ass_img.to(device).to(th.float32)\n",
    "        \n",
    "        # update D\n",
    "        if i % n_g==0:\n",
    "            lossD = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "            label.fill_(real_label)\n",
    "            lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD += lossD_real1.item()\n",
    "            lossD_real1.backward()\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "            lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD += lossD_real2.item()\n",
    "            lossD_real2.backward()\n",
    "\n",
    "            fake, _ = netg(img)\n",
    "            label.fill_(fake_label)\n",
    "            output2 = netd(fake.detach()) #需要 detach 因為不希望更新fake的參數\n",
    "            lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD += lossD_fake.item()\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake,_ = netg(img)\n",
    "            faked = th.cat((img, fake.detach()), 1)  #需要 detach 因為不希望更新fake的參數\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "            lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "            lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "            lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "        \n",
    "        if i % n_d ==0: \n",
    "            # update G\n",
    "            lossG = 0\n",
    "            optimG.zero_grad()\n",
    "            fake, A = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGD.item()\n",
    "            lossGD.backward(retain_graph=True)  ##這裡需要retain graph 因為他之後有需要fake，因此需要retain\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "            label.fill_(real_label)\n",
    "            lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGA.item()\n",
    "            lossGA.backward()\n",
    "\n",
    "            ## new tripletloss\n",
    "            _, P = netg(ass_img)\n",
    "            __, N = netg(noass_img)\n",
    "            lossf = TripletLoss(margin) #因為之前的A都沒被使用過所以不會遺失，如果是拿fake ，LossGA就需要再retain graph\n",
    "            lossTriplet =lossf(A, P, N)\n",
    "            lossG += lossTriplet.item()\n",
    "            lossTriplet.backward()\n",
    "\n",
    "    #         ## new onlinetripletloss\n",
    "    #         __, com = netg(com_img)\n",
    "    #         loss_fn = OnlineTripletLoss(margin, SemihardNegativeTripletSelector(margin))\n",
    "    #         lossTriplet,len_triplet = loss_fn(com, com_label)\n",
    "    #         lossG += lossTriplet.item()\n",
    "    #         lossTriplet.backward()\n",
    "    # #         print(lossTriplet.item(),len_triplet)\n",
    "\n",
    "    #         ## triplet loss\n",
    "    #         __, P = netg(ass_img)\n",
    "    #         __, N = netg(noass_img)\n",
    "    #         lossTriplet = F.triplet_margin_loss(A, P, N, margin = margin) #因為之前的A都沒被使用過所以不會遺失，如果是拿fake ，LossGA就需要再retain graph\n",
    "    #         lossG += lossTriplet.item()\n",
    "    #         lossTriplet.backward()\n",
    "    # #         if i%10==0:\n",
    "    # #             print(\"tripletloss \",lossTriplet.item())\n",
    "\n",
    "            ## tripletloss no negative\n",
    "    #         N_plus = th.zeros((A.size()), requires_grad=False).to(device)\n",
    "    #         lossTriplet_AP = F.triplet_margin_loss(A, P, N_plus, margin = margin)\n",
    "    #         lossG += lossTriplet_AP.item()\n",
    "    #         lossTriplet += lossTriplet_AP\n",
    "    #         lossTriplet.backward()\n",
    "\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG  Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from PixelDT code刪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T15:13:36.331308Z",
     "start_time": "2019-04-13T08:01:32.315792Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir)\n",
    "\n",
    "iteration = 0\n",
    "lr = 0.0002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 128\n",
    "\n",
    "real_label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "fake_label = th.ones((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=lr)\n",
    "optimD = optim.Adam(netd.parameters(), lr=lr)\n",
    "optimA = optim.Adam(neta.parameters(), lr=lr)\n",
    "\n",
    "print('Training starts')\n",
    "while iteration < 40000:\n",
    "    ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "    ass_label = ass_label.to(device).to(th.float32)\n",
    "    noass_label = noass_label.to(device).to(th.float32)\n",
    "    img = img.to(device).to(th.float32)\n",
    "    \n",
    "    # update D\n",
    "    lossD = 0\n",
    "    optimD.zero_grad()\n",
    "    output = netd(ass_label)\n",
    "#     label.fill_(real_label)\n",
    "    lossD_real1 = F.binary_cross_entropy(output, real_label)\n",
    "#     lossD += lossD_real1.item()\n",
    "#     lossD_real1.backward()\n",
    "\n",
    "#     label.fill_(real_label)\n",
    "    output1 = netd(noass_label)\n",
    "    lossD_real2 = F.binary_cross_entropy(output1, real_label)\n",
    "#     lossD += lossD_real2.item()\n",
    "#     lossD_real2.backward()\n",
    "\n",
    "    fake = netg(img).detach()\n",
    "#     label.fill_(fake_label)\n",
    "    output2 = netd(fake)\n",
    "    lossD_fake = F.binary_cross_entropy(output2, fake_label)\n",
    "#     lossD += lossD_fake.item()\n",
    "#     lossD_fake.backward()\n",
    "    lossD = (lossD_real1+ lossD_real2+ lossD_fake)/3\n",
    "    lossD.backward()\n",
    "\n",
    "    lossD_item = lossD.item()\n",
    "    optimD.step()\n",
    "    \n",
    "    # update A\n",
    "    lossA = 0\n",
    "    optimA.zero_grad()\n",
    "    assd = th.cat((img, ass_label), 1)\n",
    "    noassd = th.cat((img, noass_label), 1)\n",
    "    fake = netg(img).detach()\n",
    "    faked = th.cat((img, fake), 1)\n",
    "\n",
    "#     label.fill_(real_label)\n",
    "    output = neta(assd)\n",
    "    lossA_real1 = F.binary_cross_entropy(output, real_label)\n",
    "#     lossA += lossA_real1.item()\n",
    "#     lossA_real1.backward()\n",
    "\n",
    "#     label.fill_(fake_label)\n",
    "    output1 = neta(noassd)\n",
    "    lossA_real2 = F.binary_cross_entropy(output1, fake_label)\n",
    "#     lossA += lossA_real2.item()\n",
    "#     lossA_real2.backward()\n",
    "\n",
    "#     label.fill_(fake_label)\n",
    "    output2 = neta(faked)\n",
    "    lossA_fake = F.binary_cross_entropy(output2, fake_label)\n",
    "#     lossA += lossA_fake.item()\n",
    "#     lossA_fake.backward()\n",
    "    lossA = (lossA_real1+ lossA_real2 +lossA_fake)/3\n",
    "    lossA.backward()\n",
    "    \n",
    "    lossA_item = lossA.item()\n",
    "    optimA.step()\n",
    "    \n",
    "    # update G\n",
    "    lossG = 0\n",
    "    optimG.zero_grad()\n",
    "    fake = netg(img)\n",
    "    output = netd(fake)\n",
    "#     label.fill_(real_label)\n",
    "    lossGD = F.binary_cross_entropy(output, real_label)\n",
    "#     lossG += lossGD.item()\n",
    "#     lossGD.backward(retain_graph=True)\n",
    "\n",
    "    faked = th.cat((img, fake), 1)\n",
    "    output = neta(faked)\n",
    "#     label.fill_(real_label)\n",
    "    lossGA = F.binary_cross_entropy(output, real_label)\n",
    "#     lossG += lossGA.item()\n",
    "#     lossGA.backward()\n",
    "    lossG = (lossGD + lossGA)/2\n",
    "    lossG.backward()\n",
    "    \n",
    "    lossG_item = lossG.item()\n",
    "    optimG.step()\n",
    "\n",
    "    iteration += 1\n",
    "    \n",
    "    if iteration % 20 == 0:\n",
    "        with th.no_grad():\n",
    "            netg.eval()  #切換\n",
    "            fake = netg(img) \n",
    "            netg.train() #切換回去\n",
    "        fake = (fake + 1) / 2 * 255\n",
    "        real = (ass_label + 1) / 2 * 255\n",
    "        ori = (img + 1) / 2 * 255\n",
    "        al = th.cat((fake, real, ori), 2)\n",
    "        display = make_grid(al, 20).cpu().numpy()\n",
    "        if win1 is None:\n",
    "            win1 = vis.image(display,\n",
    "                             opts=dict(title=\"train\", caption='train'))\n",
    "        else:\n",
    "            vis.image(display, win=win1)\n",
    "    if iteration % 5000 == 0 or iteration==500:\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_Name+'/snapshot'+ Model_Name +'_%d.t7' % iteration)\n",
    "        with open(Model_Name+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('iter = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            iteration, lossG_item, lossA_item, lossD_item\n",
    "        ))\n",
    "        print('iter = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            iteration, lossG_item, lossA_item, lossD_item\n",
    "        ))\n",
    "    if iteration % 5000==0 or iteration==10 or iteration==500:\n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[iteration, iteration,\n",
    "                                        iteration]]),\n",
    "                           Y=np.array([[lossG_item, lossA_item, lossD_item]]),\n",
    "                           opts=dict(\n",
    "                               title='GaitGAN',\n",
    "                               ylabel='loss',\n",
    "                               xlabel='iterations',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[iteration, iteration,\n",
    "                                  iteration]]),\n",
    "                     Y=np.array([[lossG_item, lossA_item, lossD_item]]),\n",
    "                     win=win,\n",
    "                     update='append')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-12T16:51:42.619Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_con= 10 ,n_ang= 11\n",
      "target =  090\n",
      "write parameter log...\n",
      "Training starts\n",
      "Epoch = 2, ErrG = 3.752237558364868, ErrA = -1.8985527356465657, ErrD = -3.5465147892634072\n",
      "Epoch = 4, ErrG = 5.705430269241333, ErrA = -2.807316621144613, ErrD = -5.611066023508708\n",
      "Epoch = 6, ErrG = 4.896071195602417, ErrA = -3.07237180074056, ErrD = -6.601948658625285\n",
      "Epoch = 8, ErrG = 8.87260103225708, ErrA = -4.403616587320964, ErrD = -9.87374464670817\n",
      "Epoch = 10, ErrG = 7.947443723678589, ErrA = -4.1470723152160645, ErrD = -8.064921021461487\n",
      "Epoch = 12, ErrG = 1.238088607788086, ErrA = -4.921435038248698, ErrD = -4.710442225138347\n",
      "Epoch = 14, ErrG = 0.5194168090820312, ErrA = -5.6255998611450195, ErrD = -5.323735237121582\n",
      "Epoch = 16, ErrG = 6.348399639129639, ErrA = -7.5277970631917315, ErrD = -7.016017913818359\n",
      "Epoch = 18, ErrG = 0.8842306137084961, ErrA = -7.25491460164388, ErrD = -7.062686284383138\n",
      "Epoch = 20, ErrG = 0.0035724639892578125, ErrA = -7.803771336873372, ErrD = -7.927731831868489\n",
      "Epoch = 22, ErrG = 1.2339839935302734, ErrA = -8.73021380106608, ErrD = -8.565280278523764\n",
      "Epoch = 24, ErrG = 1.2255611419677734, ErrA = -9.72688357035319, ErrD = -9.670048395792643\n",
      "Epoch = 26, ErrG = 0.15535354614257812, ErrA = -10.620674769083658, ErrD = -10.38158925374349\n",
      "Epoch = 28, ErrG = 4.176642417907715, ErrA = -12.133074442545572, ErrD = -10.76137606302897\n",
      "Epoch = 30, ErrG = 4.427227973937988, ErrA = -12.199844360351562, ErrD = -12.158490498860678\n",
      "Epoch = 32, ErrG = 3.9799652099609375, ErrA = -14.347677866617838, ErrD = -14.839614232381185\n",
      "Epoch = 34, ErrG = 3.2736902236938477, ErrA = -14.609235127766928, ErrD = -15.713889439900717\n",
      "Epoch = 36, ErrG = 5.752946853637695, ErrA = -15.246897379557291, ErrD = -16.766225179036457\n",
      "Epoch = 38, ErrG = 1.6820240020751953, ErrA = -16.648352305094402, ErrD = -17.338180541992188\n",
      "Epoch = 40, ErrG = 10.724223136901855, ErrA = -17.55199940999349, ErrD = -20.6812006632487\n",
      "Epoch = 42, ErrG = 10.82100772857666, ErrA = -17.30973943074544, ErrD = -19.197577158610027\n",
      "Epoch = 44, ErrG = 6.142007827758789, ErrA = -19.585198720296223, ErrD = -21.98925272623698\n",
      "Epoch = 46, ErrG = 4.614898681640625, ErrA = -20.6905943552653, ErrD = -21.187957763671875\n",
      "Epoch = 48, ErrG = 14.935154914855957, ErrA = -20.511363983154297, ErrD = -22.849955240885418\n",
      "Epoch = 50, ErrG = 8.494245529174805, ErrA = -23.74848810831706, ErrD = -31.115222930908203\n",
      "Epoch = 52, ErrG = 9.508050918579102, ErrA = -24.220165252685547, ErrD = -25.16811243693034\n",
      "Epoch = 54, ErrG = 9.283670425415039, ErrA = -26.20640691121419, ErrD = -32.25322977701823\n",
      "Epoch = 56, ErrG = 13.134300231933594, ErrA = -23.950950622558594, ErrD = -31.37301762898763\n",
      "Epoch = 58, ErrG = 16.310850143432617, ErrA = -27.30752182006836, ErrD = -35.64556376139323\n",
      "Epoch = 60, ErrG = 9.128986358642578, ErrA = -25.83734639485677, ErrD = -38.91717656453451\n",
      "Epoch = 62, ErrG = 8.424102783203125, ErrA = -29.642303466796875, ErrD = -37.35610580444336\n",
      "Epoch = 64, ErrG = 25.559022903442383, ErrA = -31.677706400553387, ErrD = -32.23673756917318\n",
      "Epoch = 66, ErrG = 22.559337615966797, ErrA = -30.57117462158203, ErrD = -36.400753021240234\n",
      "Epoch = 68, ErrG = 15.052131652832031, ErrA = -35.96826934814453, ErrD = -46.180502573649086\n",
      "Epoch = 70, ErrG = -0.43895721435546875, ErrA = -32.98573303222656, ErrD = -43.57259877522787\n"
     ]
    }
   ],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0\n",
    "margin = 0\n",
    "n_g = 1\n",
    "n_d = 2\n",
    "clip = 0.1\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.RMSprop(netg.parameters(), lr=glr/2)\n",
    "optimD = optim.RMSprop(netd.parameters(), lr=dlr/3)\n",
    "optimA = optim.RMSprop(neta.parameters(), lr=dlr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={},clip={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp, clip))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "\n",
    "        # update D\n",
    "        if i % n_g==0:\n",
    "            lossD = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "    #         label.fill_(real_label)\n",
    "    #         lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD_real1 = -th.mean(output)\n",
    "            lossD += lossD_real1.item()\n",
    "            lossD_real1.backward()\n",
    "\n",
    "    #         label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "    #         lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD_real2 = -th.mean(output1)\n",
    "            lossD += lossD_real2.item()\n",
    "            lossD_real2.backward()\n",
    "\n",
    "            fake = netg(img).detach()\n",
    "    #         label.fill_(fake_label)\n",
    "            output2 = netd(fake)\n",
    "    #         lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD_fake = th.mean(output2)\n",
    "            lossD += lossD_fake.item()\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "            for p in netd.parameters():\n",
    "                p.data.clamp_(-clip, clip)\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake = netg(img).detach()\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "    #         label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "    #         lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA_real1 = -th.mean(output1)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "    #         label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "    #         lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA_real2 = th.mean(output)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "    #         label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "    #         lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA_fake = th.mean(output)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "\n",
    "            for p in neta.parameters():\n",
    "                p.data.clamp_(-clip, clip)\n",
    "\n",
    "        # update G\n",
    "        if i % n_d == 0:\n",
    "            lossG = 0\n",
    "            optimG.zero_grad()\n",
    "            fake = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "#             label.fill_(real_label)\n",
    "#             lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossGD = -th.mean(output)\n",
    "            lossG += lossGD.item()\n",
    "            lossGD.backward(retain_graph=True)\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "#             label.fill_(real_label)\n",
    "#             lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossGA = -th.mean(output)\n",
    "            lossG += lossGA.item()\n",
    "            lossGA.backward()\n",
    "\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    \n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-02T17:54:50.674179Z",
     "start_time": "2019-06-02T12:25:08.423036Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad, Variable\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 10\n",
    "beta1 = 0\n",
    "beta2 = 0.999\n",
    "margin = 0\n",
    "n_g = 0\n",
    "n_d = 5\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr, betas=(beta1, beta2))\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "# optimG = optim.RMSprop(netg.parameters(), lr=lr)\n",
    "# optimD = optim.RMSprop(netd.parameters(), lr=lr)\n",
    "# optimA = optim.RMSprop(neta.parameters(), lr=lr)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "#     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    alpha = th.rand((batchSize, 1, 1, 1)).to(device).to(th.float32)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "#     fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = grad(outputs=d_interpolates, \n",
    "                     inputs=interpolates, \n",
    "                     grad_outputs=th.ones([real_samples.shape[0],1]).to(device).requires_grad_(False),\n",
    "#                      grad_outputs = fake,\n",
    "                     create_graph=True, \n",
    "                     retain_graph=True, \n",
    "                     only_inputs=True)[0]\n",
    "#     fake = Variable((real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "#     # Get gradient w.r.t. interpolates\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=d_interpolates,\n",
    "#         inputs=interpolates,\n",
    "#         grad_outputs=fake,\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True,\n",
    "#     )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        \n",
    "        # update D\n",
    "        if i % n_g==0:\n",
    "            lossD = 0\n",
    "            lossD_ = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "    #         label.fill_(real_label)\n",
    "    #         lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD_real1 = -th.mean(output)\n",
    "            lossD_ += lossD_real1\n",
    "            lossD += lossD_real1.item()\n",
    "    #         lossD_real1.backward()\n",
    "\n",
    "    #         label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "    #         lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD_real2 = -th.mean(output1)\n",
    "            lossD_ += lossD_real2\n",
    "            lossD += lossD_real2.item()\n",
    "    #         lossD_real2.backward()\n",
    "\n",
    "            fake = netg(img).detach()\n",
    "    #         label.fill_(fake_label)\n",
    "            output2 = netd(fake)\n",
    "    #         lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD_fake = th.mean(output2)\n",
    "            lossD_ += lossD_fake\n",
    "            lossD += lossD_fake.item()\n",
    "            gradient_penalty = compute_gradient_penalty(netd, ass_label.data, fake.data)\n",
    "    #         lossD_fake.backward()\n",
    "            lossD_ = lossD_/3 + lambda_gp * gradient_penalty\n",
    "            lossD_.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            lossA_ = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake = netg(img).detach()\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "    #         label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "    #         lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA_real1 = -th.mean(output1)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_ += lossA_real1\n",
    "    #         lossA_real1.backward()\n",
    "\n",
    "    #         label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "    #         lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA_real2 = th.mean(output)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_ += lossA_real2\n",
    "    #         lossA_real2.backward()\n",
    "\n",
    "    #         label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "    #         lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA_fake = th.mean(output)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_ += lossA_fake\n",
    "    #         lossA_fake.backward()\n",
    "            gradient_penalty = compute_gradient_penalty(neta, assd.data, faked.data)\n",
    "            lossA_ = lossA_/3 + lambda_gp * gradient_penalty\n",
    "\n",
    "            lossA_.backward()\n",
    "            optimA.step()\n",
    "    \n",
    "            \n",
    "        # update G\n",
    "        if i % n_d == 0:\n",
    "            lossG = 0\n",
    "            lossG_ = 0\n",
    "            optimG.zero_grad()\n",
    "            fake = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "#             label.fill_(real_label)\n",
    "#             lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossGD = -th.mean(output)\n",
    "            lossG += lossGD.item()\n",
    "            lossG_ += lossGD\n",
    "#             lossGD.backward(retain_graph=True)\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "#             label.fill_(real_label)\n",
    "#             lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossGA = -th.mean(output)\n",
    "            lossG += lossGA.item()\n",
    "            lossG_ += lossGA\n",
    "#             lossGA.backward()\n",
    "            lossG_ = lossG_/2\n",
    "            lossG_.backward(retain_graph=True)\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))           \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA GaitGAN (hing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T01:02:04.206828Z",
     "start_time": "2019-08-13T16:34:01.153341Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "n_con= 10 ,n_ang= 11\n",
      "target =  090\n",
      "write parameter log...\n",
      "Training starts\n",
      "Epoch = 2, ErrG = 1.616885781288147, ErrA = 0.45645807186762494, ErrD = 0.0, Gattn=-0.0020963058341294527, Dattn=0.0011602682061493397, Aattn=-0.001853231806308031\n",
      "Epoch = 4, ErrG = 0.8820965886116028, ErrA = 0.3970751961072286, ErrD = 0.17267785842219988, Gattn=-0.00410621939226985, Dattn=0.0021039096172899008, Aattn=-0.002656357828527689\n",
      "Epoch = 6, ErrG = 1.8386868834495544, ErrA = 0.494064340988795, ErrD = 0.13149824738502502, Gattn=-0.00606911163777113, Dattn=0.006608845666050911, Aattn=-0.003808614332228899\n",
      "Epoch = 8, ErrG = 0.9222525954246521, ErrA = 0.6054384907086691, ErrD = 0.3740019624431928, Gattn=-0.008033267222344875, Dattn=0.014804177917540073, Aattn=-0.00481810700148344\n",
      "Epoch = 10, ErrG = 1.367496907711029, ErrA = 0.5269833107789358, ErrD = 0.290728231271108, Gattn=-0.009626723825931549, Dattn=0.023283688351511955, Aattn=-0.006453025620430708\n",
      "Epoch = 12, ErrG = 0.8693040013313293, ErrA = 0.5524015923341116, ErrD = 0.5603670570999384, Gattn=-0.01057243999093771, Dattn=0.027751047164201736, Aattn=-0.009476941078901291\n",
      "Epoch = 14, ErrG = -0.14872199296951294, ErrA = 0.557500292857488, ErrD = 0.3864883004377286, Gattn=-0.011113306507468224, Dattn=0.030719995498657227, Aattn=-0.014220676384866238\n",
      "Epoch = 16, ErrG = 1.4919919967651367, ErrA = 0.5566562289992968, ErrD = 0.4847428910434246, Gattn=-0.011548502370715141, Dattn=0.033613789826631546, Aattn=-0.019129015505313873\n",
      "Epoch = 18, ErrG = 1.0632918328046799, ErrA = 0.6540036102135977, ErrD = 0.04604026675224304, Gattn=-0.011912415735423565, Dattn=0.03617110103368759, Aattn=-0.02353302575647831\n",
      "Epoch = 20, ErrG = 2.0552967190742493, ErrA = 0.7560162544250488, ErrD = 1.4290135701497395, Gattn=-0.01223099697381258, Dattn=0.03850454092025757, Aattn=-0.02780643291771412\n",
      "Epoch = 22, ErrG = 0.7003284394741058, ErrA = 0.4711429327726364, ErrD = 0.35241440931955975, Gattn=-0.012522879987955093, Dattn=0.04055321216583252, Aattn=-0.03071741946041584\n",
      "Epoch = 24, ErrG = 0.4169808551669121, ErrA = 0.310110275944074, ErrD = 0.5655981786549091, Gattn=-0.012789478525519371, Dattn=0.04250336438417435, Aattn=-0.03326668217778206\n",
      "Epoch = 26, ErrG = 1.246729999780655, ErrA = 0.4107878903547923, ErrD = 0.2921422515064478, Gattn=-0.013091505505144596, Dattn=0.04428192228078842, Aattn=-0.035063885152339935\n",
      "Epoch = 28, ErrG = 1.4659357070922852, ErrA = 0.5513113786776861, ErrD = 0.1680365273108085, Gattn=-0.013307839632034302, Dattn=0.04599299281835556, Aattn=-0.036884237080812454\n",
      "Epoch = 30, ErrG = 1.091233879327774, ErrA = 0.3650950367252032, ErrD = 0.4396693706512451, Gattn=-0.013575942255556583, Dattn=0.04751080647110939, Aattn=-0.038689229637384415\n",
      "Epoch = 32, ErrG = 0.12466956675052643, ErrA = 0.5105727314949036, ErrD = 0.3646339178085327, Gattn=-0.013774172402918339, Dattn=0.04893404245376587, Aattn=-0.040347300469875336\n",
      "Epoch = 34, ErrG = 1.5914018154144287, ErrA = 0.30496473610401154, ErrD = 0.27802370861172676, Gattn=-0.01389763318002224, Dattn=0.050255920737981796, Aattn=-0.04221775755286217\n",
      "Epoch = 36, ErrG = 1.5172830820083618, ErrA = 0.2515706717967987, ErrD = 0.0380482350786527, Gattn=-0.014044993557035923, Dattn=0.051497917622327805, Aattn=-0.04380660876631737\n",
      "Epoch = 38, ErrG = 1.7200924158096313, ErrA = 0.22211415010193983, ErrD = 0.08384087247153123, Gattn=-0.014172205701470375, Dattn=0.0527067631483078, Aattn=-0.04527642950415611\n",
      "Epoch = 40, ErrG = 1.6339636445045471, ErrA = 0.2437406132618586, ErrD = 0.3266728265831868, Gattn=-0.014271317981183529, Dattn=0.053786229342222214, Aattn=-0.04659847170114517\n",
      "Epoch = 42, ErrG = 1.6910986006259918, ErrA = 0.3890808920065562, ErrD = 0.024686286846796673, Gattn=-0.014358826912939548, Dattn=0.054800622165203094, Aattn=-0.04783415049314499\n",
      "Epoch = 44, ErrG = 1.6892655491828918, ErrA = 0.202539823949337, ErrD = 0.04482949525117874, Gattn=-0.014448720030486584, Dattn=0.05571967363357544, Aattn=-0.04964439570903778\n",
      "Epoch = 46, ErrG = 1.912011742591858, ErrA = 0.1295910651485125, ErrD = 0.0, Gattn=-0.014618213288486004, Dattn=0.056597527116537094, Aattn=-0.050880786031484604\n",
      "Epoch = 48, ErrG = 1.2132985591888428, ErrA = 0.27056359748045605, ErrD = 0.19440686702728271, Gattn=-0.014698122628033161, Dattn=0.05752871185541153, Aattn=-0.052175261080265045\n",
      "Epoch = 50, ErrG = 1.754837989807129, ErrA = 0.2616424945493539, ErrD = 0.024745443214972813, Gattn=-0.014772522263228893, Dattn=0.058370355516672134, Aattn=-0.05358158424496651\n",
      "Epoch = 52, ErrG = 1.9033775329589844, ErrA = 0.2955522984266281, ErrD = 0.0, Gattn=-0.014837208203971386, Dattn=0.05912065505981445, Aattn=-0.05471421405673027\n",
      "Epoch = 54, ErrG = 1.7243410348892212, ErrA = 0.17703012625376383, ErrD = 0.0, Gattn=-0.01477934792637825, Dattn=0.059718240052461624, Aattn=-0.05593831092119217\n",
      "Epoch = 56, ErrG = 3.092637538909912, ErrA = 0.10257316132386525, ErrD = 0.0, Gattn=-0.015143629163503647, Dattn=0.05979638919234276, Aattn=-0.056551020592451096\n",
      "Epoch = 58, ErrG = 1.6613982915878296, ErrA = 0.21352893362442651, ErrD = 0.013430840025345484, Gattn=-0.015268995426595211, Dattn=0.05978595092892647, Aattn=-0.057573121041059494\n",
      "Epoch = 60, ErrG = 1.1689020693302155, ErrA = 0.36166803787151974, ErrD = 0.014798656726876894, Gattn=-0.015410678461194038, Dattn=0.06018635258078575, Aattn=-0.05839770659804344\n",
      "Epoch = 62, ErrG = 1.900496482849121, ErrA = 0.14489284654458365, ErrD = 0.0004004891961812973, Gattn=-0.015555601567029953, Dattn=0.060986436903476715, Aattn=-0.05953051894903183\n",
      "Epoch = 64, ErrG = 1.316615879535675, ErrA = 0.16094951828320822, ErrD = 0.025463392337163288, Gattn=-0.01569145917892456, Dattn=0.061737265437841415, Aattn=-0.06054968014359474\n",
      "Epoch = 66, ErrG = 1.2768854796886444, ErrA = 0.20808499803145727, ErrD = 0.0010603225479523342, Gattn=-0.01577817089855671, Dattn=0.06234004348516464, Aattn=-0.06153073534369469\n",
      "Epoch = 68, ErrG = 1.508867084980011, ErrA = 0.28129176795482635, ErrD = 0.015395810206731161, Gattn=-0.01586407981812954, Dattn=0.0631316602230072, Aattn=-0.06280267983675003\n",
      "Epoch = 70, ErrG = 2.1938823461532593, ErrA = 0.298864151040713, ErrD = 0.08287627498308818, Gattn=-0.015920545905828476, Dattn=0.06393111497163773, Aattn=-0.06415262073278427\n",
      "Epoch = 72, ErrG = 1.522874653339386, ErrA = 0.18328183765212694, ErrD = 0.002005600060025851, Gattn=-0.01594983972609043, Dattn=0.06471497565507889, Aattn=-0.06532960385084152\n",
      "Epoch = 74, ErrG = 1.8900225758552551, ErrA = 0.24798842209080854, ErrD = 0.03611055761575699, Gattn=-0.01596817746758461, Dattn=0.06537503749132156, Aattn=-0.06647075712680817\n",
      "Epoch = 76, ErrG = 1.990363895893097, ErrA = 0.20856106032927832, ErrD = 0.009559824441870054, Gattn=-0.016016360372304916, Dattn=0.06603233516216278, Aattn=-0.06760124862194061\n",
      "Epoch = 78, ErrG = 0.8451555222272873, ErrA = 0.2396964132785797, ErrD = 0.005948591977357864, Gattn=-0.0160383228212595, Dattn=0.06667746603488922, Aattn=-0.06858029961585999\n",
      "Epoch = 80, ErrG = 1.2510727047920227, ErrA = 0.23846202592055002, ErrD = 0.0235667626063029, Gattn=-0.01606968604028225, Dattn=0.06733575463294983, Aattn=-0.06946413964033127\n",
      "Epoch = 82, ErrG = 1.6043267846107483, ErrA = 0.20054126841326556, ErrD = 0.0, Gattn=-0.016070431098341942, Dattn=0.0679769366979599, Aattn=-0.07078073173761368\n",
      "Epoch = 84, ErrG = 1.8451082706451416, ErrA = 0.1968656654159228, ErrD = 0.010372693339983622, Gattn=-0.01610558107495308, Dattn=0.06867942959070206, Aattn=-0.07217864692211151\n",
      "Epoch = 86, ErrG = 1.7685084342956543, ErrA = 0.2917580393453439, ErrD = 0.03677094976107279, Gattn=-0.016143159940838814, Dattn=0.069162517786026, Aattn=-0.0731615200638771\n",
      "Epoch = 88, ErrG = 1.6309491395950317, ErrA = 0.17993451716999212, ErrD = 0.0, Gattn=-0.01614966243505478, Dattn=0.06993653625249863, Aattn=-0.07414605468511581\n",
      "Epoch = 90, ErrG = 1.589820146560669, ErrA = 0.07270340373118718, ErrD = 0.01057171014448007, Gattn=-0.016158651560544968, Dattn=0.07051980495452881, Aattn=-0.07546481490135193\n",
      "Epoch = 92, ErrG = 1.8140882849693298, ErrA = 0.1965616593758265, ErrD = 0.004698656499385834, Gattn=-0.016175659373402596, Dattn=0.07094758003950119, Aattn=-0.07637723535299301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 94, ErrG = 1.6275362968444824, ErrA = 0.12814308951298395, ErrD = 0.0, Gattn=-0.016189202666282654, Dattn=0.07154933363199234, Aattn=-0.07755158096551895\n",
      "Epoch = 96, ErrG = 1.6325467824935913, ErrA = 0.13576739033063254, ErrD = 0.0, Gattn=-0.01624268665909767, Dattn=0.0722448006272316, Aattn=-0.07858452945947647\n",
      "Epoch = 98, ErrG = 1.6740132570266724, ErrA = 0.14139717817306519, ErrD = 0.023082298537095387, Gattn=-0.01623196341097355, Dattn=0.07272286713123322, Aattn=-0.07957756519317627\n",
      "Epoch = 100, ErrG = 1.3588250875473022, ErrA = 0.20691749826073647, ErrD = 0.009358682359258333, Gattn=-0.01625732146203518, Dattn=0.07334950566291809, Aattn=-0.08057785034179688\n",
      "Epoch = 102, ErrG = 2.0385136008262634, ErrA = 0.2686113491654396, ErrD = 0.01747746703525384, Gattn=-0.016284307464957237, Dattn=0.07384389638900757, Aattn=-0.08147003501653671\n",
      "Epoch = 104, ErrG = 1.8831560611724854, ErrA = 0.18347068627675375, ErrD = 0.047291224201520286, Gattn=-0.016303449869155884, Dattn=0.07440569996833801, Aattn=-0.08238450437784195\n",
      "Epoch = 106, ErrG = 2.0447847843170166, ErrA = 0.1498469766229391, ErrD = 0.02929440140724182, Gattn=-0.01634334772825241, Dattn=0.07513470947742462, Aattn=-0.08329839259386063\n",
      "Epoch = 108, ErrG = 1.8486194610595703, ErrA = 0.20480469365914664, ErrD = 0.45923463503519696, Gattn=-0.016398673877120018, Dattn=0.07543615996837616, Aattn=-0.08414874970912933\n",
      "Epoch = 110, ErrG = 1.762040138244629, ErrA = 0.15714848538239798, ErrD = 0.013453397899866104, Gattn=-0.01643478125333786, Dattn=0.07613058388233185, Aattn=-0.08519230037927628\n",
      "Epoch = 112, ErrG = 1.224352478981018, ErrA = 0.1661472568909327, ErrD = 0.18275034427642822, Gattn=-0.016456134617328644, Dattn=0.07660391926765442, Aattn=-0.08620477467775345\n",
      "Epoch = 114, ErrG = 1.555786520242691, ErrA = 0.22558281694849333, ErrD = 0.0065587299565474195, Gattn=-0.01649167388677597, Dattn=0.07732535898685455, Aattn=-0.0870208814740181\n",
      "Epoch = 116, ErrG = 1.8012850880622864, ErrA = 0.10904762024680774, ErrD = 0.04688386619091034, Gattn=-0.0165349580347538, Dattn=0.07757418602705002, Aattn=-0.08798568695783615\n",
      "Epoch = 118, ErrG = 1.7461474537849426, ErrA = 0.2312546173731486, ErrD = 0.033102126171191536, Gattn=-0.016580402851104736, Dattn=0.07802985608577728, Aattn=-0.08888067305088043\n",
      "Epoch = 120, ErrG = 1.7953828573226929, ErrA = 0.10377363488078117, ErrD = 0.03166959683100382, Gattn=-0.016636891290545464, Dattn=0.07873024791479111, Aattn=-0.08965209126472473\n",
      "Epoch = 122, ErrG = 1.9991329908370972, ErrA = 0.1315860797961553, ErrD = 0.10058188438415527, Gattn=-0.016668369993567467, Dattn=0.07932441681623459, Aattn=-0.09060468524694443\n",
      "Epoch = 124, ErrG = 2.0624685287475586, ErrA = 0.13878958920637766, ErrD = 0.07381272315979004, Gattn=-0.0167127326130867, Dattn=0.07981465011835098, Aattn=-0.09160850942134857\n",
      "Epoch = 126, ErrG = 1.7027920484542847, ErrA = 0.15244082858165106, ErrD = 0.06018479665120443, Gattn=-0.01678304560482502, Dattn=0.08025789260864258, Aattn=-0.09235498309135437\n",
      "Epoch = 128, ErrG = 1.8436155319213867, ErrA = 0.1355083342641592, ErrD = 0.007025475303332011, Gattn=-0.01680680550634861, Dattn=0.08088299632072449, Aattn=-0.09339754283428192\n",
      "Epoch = 130, ErrG = 1.6432104110717773, ErrA = 0.11577371259530385, ErrD = 0.0019446201622486115, Gattn=-0.01684211567044258, Dattn=0.08141423016786575, Aattn=-0.09407205879688263\n",
      "Epoch = 132, ErrG = 1.7724629640579224, ErrA = 0.13180458235243955, ErrD = 0.0, Gattn=-0.016872063279151917, Dattn=0.08197032660245895, Aattn=-0.09506535530090332\n",
      "Epoch = 134, ErrG = 1.3562694191932678, ErrA = 0.12850994182129702, ErrD = 0.005468651652336121, Gattn=-0.01691213995218277, Dattn=0.08257769793272018, Aattn=-0.09621912240982056\n",
      "Epoch = 136, ErrG = 1.436037540435791, ErrA = 0.10405518611272176, ErrD = 0.3381462097167969, Gattn=-0.016937725245952606, Dattn=0.08309649676084518, Aattn=-0.09715083986520767\n",
      "Epoch = 138, ErrG = 1.187592774629593, ErrA = 0.1724712010473013, ErrD = 0.014533691108226776, Gattn=-0.016992097720503807, Dattn=0.08373846858739853, Aattn=-0.09804754704236984\n",
      "Epoch = 140, ErrG = 0.7525470554828644, ErrA = 0.11561543494462967, ErrD = 0.45125722885131836, Gattn=-0.017017388716340065, Dattn=0.08427631855010986, Aattn=-0.099033884704113\n",
      "Epoch = 142, ErrG = 2.030392050743103, ErrA = 0.1598843497534593, ErrD = 0.12384617328643799, Gattn=-0.01704714447259903, Dattn=0.08493447303771973, Aattn=-0.09990622103214264\n",
      "Epoch = 144, ErrG = 1.8405611515045166, ErrA = 0.21521369616190592, ErrD = 0.007975533604621887, Gattn=-0.017082801088690758, Dattn=0.08555816859006882, Aattn=-0.10070901364088058\n",
      "Epoch = 146, ErrG = 1.5225746631622314, ErrA = 0.15436704580982527, ErrD = 0.00222883311410745, Gattn=-0.01710626669228077, Dattn=0.08590143173933029, Aattn=-0.10172326117753983\n",
      "Epoch = 148, ErrG = 1.4876752495765686, ErrA = 0.18544167776902518, ErrD = 0.06831513345241547, Gattn=-0.01712709292769432, Dattn=0.0865362286567688, Aattn=-0.10269206017255783\n",
      "Epoch = 150, ErrG = 1.7716654539108276, ErrA = 0.1004725992679596, ErrD = 0.013711754232645035, Gattn=-0.017165210098028183, Dattn=0.08692893385887146, Aattn=-0.10351727157831192\n",
      "Epoch = 152, ErrG = 1.6373718976974487, ErrA = 0.22685223817825317, ErrD = 0.006686677535374959, Gattn=-0.01719415746629238, Dattn=0.08745983242988586, Aattn=-0.10438382625579834\n",
      "Epoch = 154, ErrG = 1.1492312252521515, ErrA = 0.3118782378733158, ErrD = 0.04434025784333547, Gattn=-0.017226096242666245, Dattn=0.0878845825791359, Aattn=-0.10518783330917358\n",
      "Epoch = 156, ErrG = 1.8467559218406677, ErrA = 0.15771111845970154, ErrD = 0.14553318917751312, Gattn=-0.017248207703232765, Dattn=0.08858757466077805, Aattn=-0.1062297523021698\n",
      "Epoch = 158, ErrG = 1.6309375166893005, ErrA = 0.13535223404566446, ErrD = 0.01840887467066447, Gattn=-0.01727178692817688, Dattn=0.08916723728179932, Aattn=-0.10728786885738373\n",
      "Epoch = 160, ErrG = 1.5844568014144897, ErrA = 0.12971446352700391, ErrD = 0.034863131741682686, Gattn=-0.017291801050305367, Dattn=0.0898829996585846, Aattn=-0.10853651911020279\n",
      "Epoch = 162, ErrG = 1.4508883357048035, ErrA = 0.10185157259305318, ErrD = 0.0, Gattn=-0.01731233298778534, Dattn=0.0901237204670906, Aattn=-0.1094495952129364\n",
      "Epoch = 164, ErrG = 1.8804395198822021, ErrA = 0.11395962536334991, ErrD = 0.019139418999354046, Gattn=-0.01731495000422001, Dattn=0.09083082526922226, Aattn=-0.11042091250419617\n",
      "Epoch = 166, ErrG = 1.676714539527893, ErrA = 0.16371694517632326, ErrD = 0.00587587555249532, Gattn=-0.017323734238743782, Dattn=0.09097076952457428, Aattn=-0.1113722026348114\n",
      "Epoch = 168, ErrG = 2.229957103729248, ErrA = 0.111104733000199, ErrD = 0.04533753792444865, Gattn=-0.01736573688685894, Dattn=0.09132558107376099, Aattn=-0.1122826486825943\n",
      "Epoch = 170, ErrG = 1.411203384399414, ErrA = 0.21214780708154043, ErrD = 0.0, Gattn=-0.017397338524460793, Dattn=0.0917251855134964, Aattn=-0.1131407618522644\n",
      "Epoch = 172, ErrG = 1.5069319009780884, ErrA = 0.14754422878225645, ErrD = 0.0, Gattn=-0.017408648505806923, Dattn=0.09228193014860153, Aattn=-0.11379701644182205\n",
      "Epoch = 174, ErrG = 1.3431962132453918, ErrA = 0.14599448442459106, ErrD = 0.024069547653198242, Gattn=-0.017432285472750664, Dattn=0.0925055518746376, Aattn=-0.11483357846736908\n",
      "Epoch = 176, ErrG = 1.4053058624267578, ErrA = 0.17065084291001162, ErrD = 4.204797248045603e-05, Gattn=-0.017435964196920395, Dattn=0.09320680052042007, Aattn=-0.11579292267560959\n",
      "Epoch = 178, ErrG = 1.1554546356201172, ErrA = 0.11741482218106587, ErrD = 0.024282976984977722, Gattn=-0.01746772974729538, Dattn=0.09379931539297104, Aattn=-0.11664849519729614\n",
      "Epoch = 180, ErrG = 1.5559977889060974, ErrA = 0.18307671447594961, ErrD = 0.030502793689568836, Gattn=-0.01748902164399624, Dattn=0.09442421048879623, Aattn=-0.11744215339422226\n",
      "Epoch = 182, ErrG = 1.5801448822021484, ErrA = 0.2377448578675588, ErrD = 0.04839261372884115, Gattn=-0.017505116760730743, Dattn=0.09497176855802536, Aattn=-0.11844299733638763\n",
      "Epoch = 184, ErrG = 1.723235547542572, ErrA = 0.20447619259357452, ErrD = 0.002880598728855451, Gattn=-0.017533477395772934, Dattn=0.09538193792104721, Aattn=-0.11942701786756516\n",
      "Epoch = 186, ErrG = 1.3507396578788757, ErrA = 0.16917761166890463, ErrD = 0.01685292273759842, Gattn=-0.01755635440349579, Dattn=0.09562674909830093, Aattn=-0.12012729793787003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 188, ErrG = 1.7993968725204468, ErrA = 0.12923596799373627, ErrD = 0.0, Gattn=-0.01757741905748844, Dattn=0.09618420153856277, Aattn=-0.12102954834699631\n",
      "Epoch = 190, ErrG = 1.8903300166130066, ErrA = 0.13277608528733253, ErrD = 0.0, Gattn=-0.017606057226657867, Dattn=0.09650719165802002, Aattn=-0.12202966213226318\n",
      "Epoch = 192, ErrG = 1.5198928117752075, ErrA = 0.14892739554246268, ErrD = 0.0, Gattn=-0.01762796752154827, Dattn=0.09716184437274933, Aattn=-0.12291866540908813\n",
      "Epoch = 194, ErrG = 1.6910642981529236, ErrA = 0.17905878772338232, ErrD = 0.0, Gattn=-0.017646370455622673, Dattn=0.09762880951166153, Aattn=-0.12369631975889206\n",
      "Epoch = 196, ErrG = 1.9219459891319275, ErrA = 0.07201228787501653, ErrD = 0.0765681800742944, Gattn=-0.017680013552308083, Dattn=0.09814928472042084, Aattn=-0.12495751678943634\n",
      "Epoch = 198, ErrG = 1.6486031413078308, ErrA = 0.1653051475683848, ErrD = 0.015836510807275772, Gattn=-0.017693473026156425, Dattn=0.0984291359782219, Aattn=-0.12568041682243347\n",
      "Epoch = 200, ErrG = 1.261144757270813, ErrA = 0.1038653055826823, ErrD = 6.374654670556386e-05, Gattn=-0.017733320593833923, Dattn=0.09900331497192383, Aattn=-0.12645094096660614\n",
      "Epoch = 202, ErrG = 1.581688940525055, ErrA = 0.13690156241257986, ErrD = 0.0, Gattn=-0.01775534637272358, Dattn=0.09967940300703049, Aattn=-0.12765458226203918\n",
      "Epoch = 204, ErrG = 1.6363433599472046, ErrA = 0.17900045091907182, ErrD = 0.02562080572048823, Gattn=-0.01776227541267872, Dattn=0.10029202699661255, Aattn=-0.12846557796001434\n",
      "Epoch = 206, ErrG = 2.3666954040527344, ErrA = 0.14623557031154633, ErrD = 0.06205623845259348, Gattn=-0.01781950518488884, Dattn=0.10260143131017685, Aattn=-0.12931859493255615\n",
      "Epoch = 208, ErrG = 2.3378591537475586, ErrA = 0.1529102362692356, ErrD = 0.0006920751184225082, Gattn=-0.017861997708678246, Dattn=0.1038501188158989, Aattn=-0.13032032549381256\n",
      "Epoch = 210, ErrG = 1.8498428463935852, ErrA = 0.06998367980122566, ErrD = 0.0, Gattn=-0.017884917557239532, Dattn=0.10420438647270203, Aattn=-0.13147668540477753\n",
      "Epoch = 212, ErrG = 2.228879690170288, ErrA = 0.0941439059873422, ErrD = 0.0, Gattn=-0.017833709716796875, Dattn=0.10483736544847488, Aattn=-0.13230806589126587\n",
      "Epoch = 214, ErrG = 1.762757420539856, ErrA = 0.03736879055698713, ErrD = 0.0, Gattn=-0.0177778210490942, Dattn=0.10513127595186234, Aattn=-0.13259588181972504\n",
      "Epoch = 216, ErrG = 2.316193103790283, ErrA = 0.09539612631003062, ErrD = 0.0, Gattn=-0.017776282504200935, Dattn=0.10578128695487976, Aattn=-0.13286899030208588\n",
      "Epoch = 218, ErrG = 2.383457899093628, ErrA = 0.05946048597494761, ErrD = 0.0, Gattn=-0.01757553033530712, Dattn=0.10564042627811432, Aattn=-0.1328253149986267\n",
      "Epoch = 220, ErrG = 2.5502933263778687, ErrA = 0.015713988492886227, ErrD = 0.0, Gattn=-0.017625609412789345, Dattn=0.1053532212972641, Aattn=-0.134102001786232\n",
      "Epoch = 222, ErrG = 2.16398823261261, ErrA = 0.004210971295833588, ErrD = 0.0, Gattn=-0.017629370093345642, Dattn=0.10575374215841293, Aattn=-0.13375955820083618\n",
      "Epoch = 224, ErrG = 1.9586261510849, ErrA = 0.012094708780447641, ErrD = 0.0, Gattn=-0.017590569332242012, Dattn=0.10575374215841293, Aattn=-0.13317064940929413\n",
      "Epoch = 226, ErrG = 2.618006467819214, ErrA = 0.0, ErrD = 0.0, Gattn=-0.017687108367681503, Dattn=0.10569008439779282, Aattn=-0.13304373621940613\n",
      "Epoch = 228, ErrG = 2.5435051918029785, ErrA = 0.00942163293560346, ErrD = 0.0, Gattn=-0.01776191033422947, Dattn=0.10538914054632187, Aattn=-0.13288243114948273\n",
      "Epoch = 230, ErrG = 3.119377851486206, ErrA = 0.0, ErrD = 0.0, Gattn=-0.017805248498916626, Dattn=0.10547379404306412, Aattn=-0.1329805552959442\n",
      "Epoch = 232, ErrG = 2.088077664375305, ErrA = 0.00034905970096588135, ErrD = 0.0, Gattn=-0.017851468175649643, Dattn=0.10547379404306412, Aattn=-0.133252814412117\n",
      "Epoch = 234, ErrG = 3.21407413482666, ErrA = 0.0, ErrD = 0.0, Gattn=-0.017834365367889404, Dattn=0.10541790723800659, Aattn=-0.13293328881263733\n",
      "Epoch = 236, ErrG = 3.05043888092041, ErrA = 0.0092134860654672, ErrD = 0.0, Gattn=-0.017902789637446404, Dattn=0.10541790723800659, Aattn=-0.1332322508096695\n",
      "Epoch = 238, ErrG = 2.305503785610199, ErrA = 0.02263788568476836, ErrD = 0.0, Gattn=-0.01785394921898842, Dattn=0.10542672127485275, Aattn=-0.1329614371061325\n",
      "Epoch = 240, ErrG = 1.7813782691955566, ErrA = 0.016034940878550213, ErrD = 0.0, Gattn=-0.017755549401044846, Dattn=0.10568016767501831, Aattn=-0.13305224478244781\n",
      "Epoch = 242, ErrG = 4.368451833724976, ErrA = 0.0027637587239344916, ErrD = 0.0, Gattn=-0.01772916130721569, Dattn=0.10580665618181229, Aattn=-0.13321053981781006\n",
      "Epoch = 244, ErrG = 4.744176983833313, ErrA = 0.0384724959731102, ErrD = 0.0, Gattn=-0.018027082085609436, Dattn=0.10580665618181229, Aattn=-0.13310691714286804\n",
      "Epoch = 246, ErrG = 3.481764793395996, ErrA = 0.02411310871442159, ErrD = 0.0, Gattn=-0.018091317266225815, Dattn=0.10580665618181229, Aattn=-0.13340544700622559\n",
      "Epoch = 248, ErrG = 5.5407631397247314, ErrA = 0.014928934474786123, ErrD = 0.0, Gattn=-0.018535561859607697, Dattn=0.10580665618181229, Aattn=-0.1330128163099289\n",
      "Epoch = 250, ErrG = 1.778733879327774, ErrA = 0.01812161256869634, ErrD = 0.48008598883946735, Gattn=-0.018825000151991844, Dattn=0.10629301518201828, Aattn=-0.13454663753509521\n",
      "Epoch = 252, ErrG = 1.5036661028862, ErrA = 0.02501898507277171, ErrD = 0.5075321992238363, Gattn=-0.01894863322377205, Dattn=0.10977044701576233, Aattn=-0.13613039255142212\n",
      "Epoch = 254, ErrG = 1.848155677318573, ErrA = 0.0, ErrD = 0.3895418594280879, Gattn=-0.01908218488097191, Dattn=0.11818744987249374, Aattn=-0.1374109387397766\n",
      "Epoch = 256, ErrG = 2.321462392807007, ErrA = 0.000461363544066747, ErrD = 0.19594370077053705, Gattn=-0.019340503960847855, Dattn=0.1258484572172165, Aattn=-0.13771118223667145\n",
      "Epoch = 258, ErrG = 6.368693232536316, ErrA = 0.009677002827326456, ErrD = 0.0, Gattn=-0.019333945587277412, Dattn=0.12862621247768402, Aattn=-0.13794314861297607\n",
      "Epoch = 260, ErrG = 3.2169600129127502, ErrA = 0.027138027052084606, ErrD = 0.0, Gattn=-0.019242022186517715, Dattn=0.12838032841682434, Aattn=-0.1380740851163864\n",
      "Epoch = 262, ErrG = 3.367063879966736, ErrA = 0.01480402797460556, ErrD = 0.0, Gattn=-0.019319631159305573, Dattn=0.12835972011089325, Aattn=-0.13859757781028748\n",
      "Epoch = 264, ErrG = 2.8438767194747925, ErrA = 0.02128720593949159, ErrD = 0.0, Gattn=-0.01938987709581852, Dattn=0.12835970520973206, Aattn=-0.13886171579360962\n",
      "Epoch = 266, ErrG = 5.426329135894775, ErrA = 0.0023849401623010635, ErrD = 0.0, Gattn=-0.01939459517598152, Dattn=0.12828311324119568, Aattn=-0.13894958794116974\n",
      "Epoch = 268, ErrG = 4.638467311859131, ErrA = 0.03813790033260981, ErrD = 0.0, Gattn=-0.019331447780132294, Dattn=0.12828311324119568, Aattn=-0.1391751766204834\n",
      "Epoch = 270, ErrG = 3.6964532136917114, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019222740083932877, Dattn=0.12828311324119568, Aattn=-0.13964110612869263\n",
      "Epoch = 272, ErrG = 3.77494478225708, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01918919011950493, Dattn=0.12828311324119568, Aattn=-0.1400122046470642\n",
      "Epoch = 274, ErrG = 3.565454363822937, ErrA = 0.00244021105269591, ErrD = 0.0, Gattn=-0.019252663478255272, Dattn=0.12828309834003448, Aattn=-0.14069049060344696\n",
      "Epoch = 276, ErrG = 3.2834553718566895, ErrA = 0.022266730666160583, ErrD = 0.0, Gattn=-0.019319545477628708, Dattn=0.12828309834003448, Aattn=-0.14171896874904633\n",
      "Epoch = 278, ErrG = 3.974162220954895, ErrA = 0.026386909186840057, ErrD = 0.0, Gattn=-0.01928745210170746, Dattn=0.12828309834003448, Aattn=-0.1419149935245514\n",
      "Epoch = 280, ErrG = 2.8455560207366943, ErrA = 0.010376290107766787, ErrD = 0.0, Gattn=-0.019191572442650795, Dattn=0.12828309834003448, Aattn=-0.14274747669696808\n",
      "Epoch = 282, ErrG = 3.5366992950439453, ErrA = 0.04869133916993936, ErrD = 0.0, Gattn=-0.019157368689775467, Dattn=0.12828309834003448, Aattn=-0.1429259181022644\n",
      "Epoch = 284, ErrG = 3.314356803894043, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01914193667471409, Dattn=0.12828309834003448, Aattn=-0.14383751153945923\n",
      "Epoch = 286, ErrG = 2.8109889030456543, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019103987142443657, Dattn=0.12828309834003448, Aattn=-0.14451169967651367\n",
      "Epoch = 288, ErrG = 3.23934268951416, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019069625064730644, Dattn=0.12828309834003448, Aattn=-0.1443607360124588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 290, ErrG = 3.092173457145691, ErrA = 0.03599401315053304, ErrD = 0.0, Gattn=-0.01902659796178341, Dattn=0.12828309834003448, Aattn=-0.14506329596042633\n",
      "Epoch = 292, ErrG = 4.209545016288757, ErrA = 0.0005994904786348343, ErrD = 0.0, Gattn=-0.01896648295223713, Dattn=0.12828309834003448, Aattn=-0.14552220702171326\n",
      "Epoch = 294, ErrG = 3.1149765253067017, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018983880057930946, Dattn=0.12828309834003448, Aattn=-0.14589282870292664\n",
      "Epoch = 296, ErrG = 3.9159406423568726, ErrA = 0.008155032992362976, ErrD = 0.0, Gattn=-0.019002065062522888, Dattn=0.12828309834003448, Aattn=-0.14655770361423492\n",
      "Epoch = 298, ErrG = 3.8342342376708984, ErrA = 0.004564343641201655, ErrD = 0.0, Gattn=-0.019022921100258827, Dattn=0.12828309834003448, Aattn=-0.14731480181217194\n",
      "Epoch = 300, ErrG = 2.722634196281433, ErrA = 0.03775585566957792, ErrD = 0.0, Gattn=-0.01900261640548706, Dattn=0.12828309834003448, Aattn=-0.14766381680965424\n",
      "Epoch = 302, ErrG = 2.7025136947631836, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01890949159860611, Dattn=0.12828309834003448, Aattn=-0.14839953184127808\n",
      "Epoch = 304, ErrG = 2.993890881538391, ErrA = 0.0047422659893830614, ErrD = 0.0, Gattn=-0.018897131085395813, Dattn=0.12828309834003448, Aattn=-0.148735910654068\n",
      "Epoch = 306, ErrG = 3.4630290269851685, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01892458274960518, Dattn=0.12828309834003448, Aattn=-0.1488872915506363\n",
      "Epoch = 308, ErrG = 4.277265548706055, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018927013501524925, Dattn=0.12828309834003448, Aattn=-0.14998146891593933\n",
      "Epoch = 310, ErrG = 3.0395214557647705, ErrA = 0.012795134137074152, ErrD = 0.0, Gattn=-0.01893114112317562, Dattn=0.12828309834003448, Aattn=-0.15100662410259247\n",
      "Epoch = 312, ErrG = 3.7208633422851562, ErrA = 0.01585260530312856, ErrD = 0.0, Gattn=-0.018972856923937798, Dattn=0.12828309834003448, Aattn=-0.1516237109899521\n",
      "Epoch = 314, ErrG = 3.4960556030273438, ErrA = 0.0, ErrD = 0.0, Gattn=-0.0189515370875597, Dattn=0.12828309834003448, Aattn=-0.15305987000465393\n",
      "Epoch = 316, ErrG = 3.9252452850341797, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01900600641965866, Dattn=0.12828309834003448, Aattn=-0.15287816524505615\n",
      "Epoch = 318, ErrG = 3.6368861198425293, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01901358738541603, Dattn=0.12828309834003448, Aattn=-0.15364432334899902\n",
      "Epoch = 320, ErrG = 4.130112171173096, ErrA = 0.013032182430227598, ErrD = 0.0, Gattn=-0.019016083329916, Dattn=0.12828309834003448, Aattn=-0.15435558557510376\n",
      "Epoch = 322, ErrG = 3.3429296016693115, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018946999683976173, Dattn=0.12828309834003448, Aattn=-0.15487976372241974\n",
      "Epoch = 324, ErrG = 3.428161859512329, ErrA = 0.006682679057121277, ErrD = 0.0, Gattn=-0.018942425027489662, Dattn=0.12828309834003448, Aattn=-0.15498191118240356\n",
      "Epoch = 326, ErrG = 3.2105963230133057, ErrA = 0.005988858640193939, ErrD = 0.0, Gattn=-0.018932035192847252, Dattn=0.12828309834003448, Aattn=-0.15628863871097565\n",
      "Epoch = 328, ErrG = 3.666051983833313, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018927056342363358, Dattn=0.12828309834003448, Aattn=-0.1563662439584732\n",
      "Epoch = 330, ErrG = 2.9217203855514526, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01888556033372879, Dattn=0.12828309834003448, Aattn=-0.15720072388648987\n",
      "Epoch = 332, ErrG = 3.762659192085266, ErrA = 0.009706670418381691, ErrD = 0.0, Gattn=-0.018856966868042946, Dattn=0.12828309834003448, Aattn=-0.1577465534210205\n",
      "Epoch = 334, ErrG = 3.7993358373641968, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01886383816599846, Dattn=0.12828309834003448, Aattn=-0.15854817628860474\n",
      "Epoch = 336, ErrG = 3.1252496242523193, ErrA = 0.0, ErrD = 0.0, Gattn=-0.0188594963401556, Dattn=0.12828309834003448, Aattn=-0.15947304666042328\n",
      "Epoch = 338, ErrG = 3.595126152038574, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018867095932364464, Dattn=0.12828309834003448, Aattn=-0.16003988683223724\n",
      "Epoch = 340, ErrG = 3.4908266067504883, ErrA = 0.00811038538813591, ErrD = 0.0, Gattn=-0.018886417150497437, Dattn=0.12828309834003448, Aattn=-0.16078566014766693\n",
      "Epoch = 342, ErrG = 3.8068790435791016, ErrA = 0.02347887059052785, ErrD = 0.0, Gattn=-0.01886541023850441, Dattn=0.12828309834003448, Aattn=-0.16155025362968445\n",
      "Epoch = 344, ErrG = 3.7413101196289062, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018783925101161003, Dattn=0.12828309834003448, Aattn=-0.16184210777282715\n",
      "Epoch = 346, ErrG = 4.083459138870239, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01876859739422798, Dattn=0.12828309834003448, Aattn=-0.16282983124256134\n",
      "Epoch = 348, ErrG = 3.750525712966919, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01876947283744812, Dattn=0.12828309834003448, Aattn=-0.16260865330696106\n",
      "Epoch = 350, ErrG = 4.4801411628723145, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018744757398962975, Dattn=0.12828309834003448, Aattn=-0.16317997872829437\n",
      "Epoch = 352, ErrG = 3.4876291751861572, ErrA = 0.002959501619140307, ErrD = 0.0, Gattn=-0.0186818428337574, Dattn=0.12828309834003448, Aattn=-0.16298505663871765\n",
      "Epoch = 354, ErrG = 4.943131923675537, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018701676279306412, Dattn=0.12828309834003448, Aattn=-0.16393987834453583\n",
      "Epoch = 356, ErrG = 3.695922017097473, ErrA = 0.01159102221330007, ErrD = 0.0, Gattn=-0.018674513325095177, Dattn=0.12828309834003448, Aattn=-0.16493108868598938\n",
      "Epoch = 358, ErrG = 4.7374773025512695, ErrA = 0.05510049064954122, ErrD = 0.0, Gattn=-0.01866353675723076, Dattn=0.12828309834003448, Aattn=-0.16540850698947906\n",
      "Epoch = 360, ErrG = 4.776337385177612, ErrA = 0.015091507385174433, ErrD = 0.0, Gattn=-0.018641550093889236, Dattn=0.12828309834003448, Aattn=-0.1652166247367859\n",
      "Epoch = 362, ErrG = 4.887560844421387, ErrA = 0.01610119640827179, ErrD = 0.0, Gattn=-0.018648549914360046, Dattn=0.12828309834003448, Aattn=-0.16503283381462097\n",
      "Epoch = 364, ErrG = 5.253208637237549, ErrA = 0.02893272042274475, ErrD = 0.0, Gattn=-0.018664412200450897, Dattn=0.12828309834003448, Aattn=-0.16631504893302917\n",
      "Epoch = 366, ErrG = 4.666177034378052, ErrA = 0.0057296038915713625, ErrD = 0.0, Gattn=-0.01870601810514927, Dattn=0.12828309834003448, Aattn=-0.16664187610149384\n",
      "Epoch = 368, ErrG = 3.9494338035583496, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01877756416797638, Dattn=0.12828309834003448, Aattn=-0.1674102246761322\n",
      "Epoch = 370, ErrG = 3.685533285140991, ErrA = 0.03115302634735902, ErrD = 0.0, Gattn=-0.018745725974440575, Dattn=0.12828309834003448, Aattn=-0.16835692524909973\n",
      "Epoch = 372, ErrG = 4.131505131721497, ErrA = 0.009139467651645342, ErrD = 0.0, Gattn=-0.018745509907603264, Dattn=0.12828309834003448, Aattn=-0.16905814409255981\n",
      "Epoch = 374, ErrG = 4.249673128128052, ErrA = 0.012705835203329722, ErrD = 0.0, Gattn=-0.01874728873372078, Dattn=0.12828309834003448, Aattn=-0.17067494988441467\n",
      "Epoch = 376, ErrG = 4.193691253662109, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01878977008163929, Dattn=0.12828309834003448, Aattn=-0.1711924821138382\n",
      "Epoch = 378, ErrG = 4.405864000320435, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018810054287314415, Dattn=0.12828309834003448, Aattn=-0.17122642695903778\n",
      "Epoch = 380, ErrG = 4.16832423210144, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018781399354338646, Dattn=0.12828309834003448, Aattn=-0.17166867852210999\n",
      "Epoch = 382, ErrG = 4.847244739532471, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01874382793903351, Dattn=0.12828309834003448, Aattn=-0.1722688227891922\n",
      "Epoch = 384, ErrG = 4.77519965171814, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01873529516160488, Dattn=0.12828309834003448, Aattn=-0.17332205176353455\n",
      "Epoch = 386, ErrG = 5.277056932449341, ErrA = 0.03243308514356613, ErrD = 0.0, Gattn=-0.018785012885928154, Dattn=0.12828309834003448, Aattn=-0.17356425523757935\n",
      "Epoch = 388, ErrG = 3.9674386978149414, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018855750560760498, Dattn=0.12828309834003448, Aattn=-0.17407041788101196\n",
      "Epoch = 390, ErrG = 4.58777379989624, ErrA = 0.004339431722958882, ErrD = 0.0, Gattn=-0.018913332372903824, Dattn=0.12828309834003448, Aattn=-0.1753636598587036\n",
      "Epoch = 392, ErrG = 4.5294013023376465, ErrA = 0.0, ErrD = 0.0, Gattn=-0.018955109640955925, Dattn=0.12828309834003448, Aattn=-0.17579001188278198\n",
      "Epoch = 394, ErrG = 5.274937629699707, ErrA = 0.0, ErrD = 0.0, Gattn=-0.0190291590988636, Dattn=0.12828309834003448, Aattn=-0.17599478363990784\n",
      "Epoch = 396, ErrG = 4.332168102264404, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01897788792848587, Dattn=0.12828309834003448, Aattn=-0.17640067636966705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 398, ErrG = 4.420372724533081, ErrA = 0.008453298980991045, ErrD = 0.0, Gattn=-0.019029056653380394, Dattn=0.12828309834003448, Aattn=-0.17681127786636353\n",
      "Epoch = 400, ErrG = 5.574413776397705, ErrA = 0.003455959881345431, ErrD = 0.0, Gattn=-0.019121088087558746, Dattn=0.12828309834003448, Aattn=-0.17690542340278625\n",
      "Epoch = 402, ErrG = 5.717231512069702, ErrA = 0.003681067998210589, ErrD = 0.0, Gattn=-0.019124507904052734, Dattn=0.12828309834003448, Aattn=-0.1768314093351364\n",
      "Epoch = 404, ErrG = 4.768608331680298, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01907649077475071, Dattn=0.12828309834003448, Aattn=-0.17736266553401947\n",
      "Epoch = 406, ErrG = 4.5890949964523315, ErrA = 0.0336657619724671, ErrD = 0.0, Gattn=-0.019104810431599617, Dattn=0.12828309834003448, Aattn=-0.17808718979358673\n",
      "Epoch = 408, ErrG = 5.096924662590027, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01908019371330738, Dattn=0.12828309834003448, Aattn=-0.17855317890644073\n",
      "Epoch = 410, ErrG = 4.457731008529663, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01906580664217472, Dattn=0.12828309834003448, Aattn=-0.1797574907541275\n",
      "Epoch = 412, ErrG = 4.55450439453125, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019047005102038383, Dattn=0.12828309834003448, Aattn=-0.17991819977760315\n",
      "Epoch = 414, ErrG = 4.250261068344116, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01904168352484703, Dattn=0.12828309834003448, Aattn=-0.1811767965555191\n",
      "Epoch = 416, ErrG = 5.335719108581543, ErrA = 0.028410737713177998, ErrD = 0.0, Gattn=-0.01906920224428177, Dattn=0.12828309834003448, Aattn=-0.18145284056663513\n",
      "Epoch = 418, ErrG = 4.886065244674683, ErrA = 0.012446423371632894, ErrD = 0.0, Gattn=-0.01906861551105976, Dattn=0.12828309834003448, Aattn=-0.1820639967918396\n",
      "Epoch = 420, ErrG = 4.879702568054199, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01906445249915123, Dattn=0.12828309834003448, Aattn=-0.182321697473526\n",
      "Epoch = 422, ErrG = 4.722676515579224, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019156504422426224, Dattn=0.12828309834003448, Aattn=-0.1833348423242569\n",
      "Epoch = 424, ErrG = 5.931902885437012, ErrA = 0.023685952027638752, ErrD = 0.0, Gattn=-0.01915082521736622, Dattn=0.12828309834003448, Aattn=-0.18446914851665497\n",
      "Epoch = 426, ErrG = 5.222594857215881, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019204726442694664, Dattn=0.12828309834003448, Aattn=-0.18482936918735504\n",
      "Epoch = 428, ErrG = 5.822993993759155, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01925777830183506, Dattn=0.12828309834003448, Aattn=-0.18533889949321747\n",
      "Epoch = 430, ErrG = 4.355859279632568, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01930452324450016, Dattn=0.12828309834003448, Aattn=-0.1865261197090149\n",
      "Epoch = 432, ErrG = 5.366777181625366, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01920970343053341, Dattn=0.12828309834003448, Aattn=-0.1868315488100052\n",
      "Epoch = 434, ErrG = 4.694804668426514, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019290369004011154, Dattn=0.12828309834003448, Aattn=-0.18738941848278046\n",
      "Epoch = 436, ErrG = 4.3195037841796875, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019326671957969666, Dattn=0.12828309834003448, Aattn=-0.18759900331497192\n",
      "Epoch = 438, ErrG = 4.757106781005859, ErrA = 0.01582432786623637, ErrD = 0.0, Gattn=-0.019338276237249374, Dattn=0.12828309834003448, Aattn=-0.1879587322473526\n",
      "Epoch = 440, ErrG = 5.87010383605957, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019272975623607635, Dattn=0.12828309834003448, Aattn=-0.18822018802165985\n",
      "Epoch = 442, ErrG = 6.186440706253052, ErrA = 0.00026096031069755554, ErrD = 0.0, Gattn=-0.019337346777319908, Dattn=0.12828309834003448, Aattn=-0.18832939863204956\n",
      "Epoch = 444, ErrG = 4.592995882034302, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01936694048345089, Dattn=0.12828309834003448, Aattn=-0.18871335685253143\n",
      "Epoch = 446, ErrG = 4.782762765884399, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019334308803081512, Dattn=0.12828309834003448, Aattn=-0.1899234801530838\n",
      "Epoch = 448, ErrG = 5.214011549949646, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01937713474035263, Dattn=0.12828309834003448, Aattn=-0.1906394213438034\n",
      "Epoch = 450, ErrG = 5.002308130264282, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01947006955742836, Dattn=0.12828309834003448, Aattn=-0.1910451501607895\n",
      "Epoch = 452, ErrG = 4.85760498046875, ErrA = 0.029102007548014324, ErrD = 0.0, Gattn=-0.0193830244243145, Dattn=0.12828309834003448, Aattn=-0.19096137583255768\n",
      "Epoch = 454, ErrG = 5.148258447647095, ErrA = 0.001657050723830859, ErrD = 0.0, Gattn=-0.01939580589532852, Dattn=0.12828309834003448, Aattn=-0.19085822999477386\n",
      "Epoch = 456, ErrG = 6.642026662826538, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019401177763938904, Dattn=0.12828309834003448, Aattn=-0.1910032480955124\n",
      "Epoch = 458, ErrG = 6.375266790390015, ErrA = 0.0431309857716163, ErrD = 0.0, Gattn=-0.01940176635980606, Dattn=0.12828309834003448, Aattn=-0.19184809923171997\n",
      "Epoch = 460, ErrG = 5.969389915466309, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019361093640327454, Dattn=0.12828309834003448, Aattn=-0.19253700971603394\n",
      "Epoch = 462, ErrG = 6.644650459289551, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01936311274766922, Dattn=0.12828309834003448, Aattn=-0.19316165149211884\n",
      "Epoch = 464, ErrG = 5.567702054977417, ErrA = 0.0, ErrD = 0.0, Gattn=-0.01939798705279827, Dattn=0.12828309834003448, Aattn=-0.19328108429908752\n",
      "Epoch = 466, ErrG = 6.747645854949951, ErrA = 0.0, ErrD = 0.0, Gattn=-0.019414296373724937, Dattn=0.12828309834003448, Aattn=-0.19339409470558167\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d2f040834018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0massd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mass_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mnoassd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoass_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mfaked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mfaked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#需要 detach 因為不希望更新fake的參數\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_ting_cv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/deep3072/Data/tingen/GaitGAN-paper/model_SAGAN1_1.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mencode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;31m#         print(\"1\",out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_ting_cv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_ting_cv/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_ting_cv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/deep3072/Data/tingen/GaitGAN-paper/spectral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_u_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/deep3072/Data/tingen/GaitGAN-paper/spectral.py\u001b[0m in \u001b[0;36m_update_u_v\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/deep3072/Data/tingen/GaitGAN-paper/spectral.py\u001b[0m in \u001b[0;36ml2normalize\u001b[0;34m(v, eps)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0ml2normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_ting_cv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;34mr\"\"\"See :func: `torch.norm`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbtrifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_ting_cv/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad, Variable\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "\n",
    "epoches = 700\n",
    "glr = 0.00001\n",
    "dlr = 0.00004\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0.9\n",
    "margin = 0\n",
    "n_g = 1\n",
    "n_d = 2\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr, betas=(beta1, beta2))\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "# optimG = optim.RMSprop(netg.parameters(), lr=lr)\n",
    "# optimD = optim.RMSprop(netd.parameters(), lr=lr)\n",
    "# optimA = optim.RMSprop(neta.parameters(), lr=lr)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "#     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    alpha = th.rand((batchSize, 1, 1, 1)).to(device).to(th.float32)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "#     fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = grad(outputs=d_interpolates, \n",
    "                     inputs=interpolates, \n",
    "                     grad_outputs=th.ones([real_samples.shape[0],1]).to(device).requires_grad_(False),\n",
    "#                      grad_outputs = fake,\n",
    "                     create_graph=True, \n",
    "                     retain_graph=True, \n",
    "                     only_inputs=True)[0]\n",
    "#     fake = Variable((real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "#     # Get gradient w.r.t. interpolates\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=d_interpolates,\n",
    "#         inputs=interpolates,\n",
    "#         grad_outputs=fake,\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True,\n",
    "#     )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "\n",
    "        if i % n_g ==0:\n",
    "            # update D\n",
    "            lossD = 0\n",
    "            lossD_ = 0\n",
    "            optimD.zero_grad()\n",
    "            d_out_assreal,dr1 = netd(ass_label)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_assreal\n",
    "            lossD += d_loss_assreal.item()\n",
    "\n",
    "            d_out_noassreal,dr2 = netd(noass_label)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 - d_out_noassreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_noassreal\n",
    "            lossD += d_loss_noassreal.item()\n",
    "\n",
    "            fake, gf1 = netg(img)\n",
    "            d_out_fake, df1 = netd(fake.detach())  #需要 detach 因為不希望更新fake的參數\n",
    "            d_loss_fake = nn.ReLU()(1.0 + d_out_fake).mean()\n",
    "\n",
    "            lossD_ += d_loss_fake\n",
    "            lossD += d_loss_fake.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(netd, ass_label.data, fake.data)\n",
    "            lossD_ = lossD_/3\n",
    "            lossD_.backward()\n",
    "            optimD.step()\n",
    "\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            lossA_ = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            faked, gf1 = netg(img)\n",
    "            faked = th.cat((img, faked.detach()), 1)  #需要 detach 因為不希望更新fake的參數\n",
    "\n",
    "            d_out_assreal,dr1 = neta(assd)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "            lossA += d_loss_assreal.item()\n",
    "            lossA_ += d_loss_assreal\n",
    "\n",
    "            d_out_noassreal,dr2 = neta(noassd)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 + d_out_noassreal).mean()\n",
    "\n",
    "            lossA_ += d_loss_noassreal\n",
    "            lossA += d_loss_noassreal.item()\n",
    "\n",
    "            d_out_faked, df3 = neta(faked)\n",
    "            d_loss_faked = nn.ReLU()(1.0 + d_out_faked).mean()\n",
    "\n",
    "            lossA_ += d_loss_faked\n",
    "            lossA += d_loss_faked.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(neta, assd.data, faked.data)\n",
    "            lossA_ = lossA_/3\n",
    "            lossA_.backward()\n",
    "            optimA.step()\n",
    "\n",
    "            \n",
    "        # update G\n",
    "        if i % n_d == 0:\n",
    "            lossG = 0\n",
    "            lossG_ = 0\n",
    "            optimG.zero_grad()\n",
    "            fake,_= netg(img)\n",
    "            g_out_fake,_ = netd(fake)\n",
    "            g_loss_fake = - g_out_fake.mean()\n",
    "\n",
    "            lossG += g_loss_fake.item()\n",
    "            lossG_ += g_loss_fake\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            g_out_faked,_ = neta(faked)\n",
    "            g_loss_faked = - g_out_faked.mean()\n",
    "            lossG += g_loss_faked.item()\n",
    "            lossG_ += g_loss_faked\n",
    "\n",
    "            lossG_ = lossG_/2\n",
    "            lossG_.backward(retain_graph=True) ##其實這裡也不需要retain graph 因為他只 backward 一次；\n",
    "            #如果爾後有要再backward，第一次就需要retain graph\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}, Gattn={}, Dattn={}, Aattn={}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3, netg.attn.gamma.item(), netd.attn.gamma.item(), neta.attn.gamma.item()\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))           \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA GaitGAN (WGAN-GP) 刪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T00:07:22.217162Z",
     "start_time": "2019-06-05T16:58:50.881140Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad, Variable\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00001\n",
    "dlr = 0.00004\n",
    "# lr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "n_critic = 0\n",
    "target = '090'\n",
    "lambda_gp = 10\n",
    "beta1 = 0\n",
    "beta2 = 0.9\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr, betas=(beta1, beta2))\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "# optimG = optim.RMSprop(netg.parameters(), lr=lr)\n",
    "# optimD = optim.RMSprop(netd.parameters(), lr=lr)\n",
    "# optimA = optim.RMSprop(neta.parameters(), lr=lr)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_critic = {}, target={},lambda_gp={} \\n'.format(\n",
    "            epoches, dlr, glr, batchSize, beta1, beta2, n_critic, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "#     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    alpha = th.rand((batchSize, 1, 1, 1)).to(device).to(th.float32)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "#     fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = grad(outputs=d_interpolates, \n",
    "                     inputs=interpolates, \n",
    "                     grad_outputs=th.ones([real_samples.shape[0],1]).to(device).requires_grad_(False),\n",
    "#                      grad_outputs = fake,\n",
    "                     create_graph=True, \n",
    "                     retain_graph=True, \n",
    "                     only_inputs=True)[0]\n",
    "#     fake = Variable((real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "#     # Get gradient w.r.t. interpolates\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=d_interpolates,\n",
    "#         inputs=interpolates,\n",
    "#         grad_outputs=fake,\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True,\n",
    "#     )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "    # while epoch < 30000:\n",
    "    #     ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        \n",
    "        \n",
    "        # update D\n",
    "        lossD = 0\n",
    "        lossD_ = 0\n",
    "        optimD.zero_grad()\n",
    "        d_out_assreal,dr1 = netd(ass_label)\n",
    "        d_loss_assreal = -th.mean(d_out_assreal)\n",
    "\n",
    "        lossD_ += d_loss_assreal\n",
    "        lossD += d_loss_assreal.item()\n",
    "\n",
    "        d_out_noassreal,dr2 = netd(noass_label)\n",
    "        d_loss_noassreal = -th.mean(d_out_noassreal)\n",
    "\n",
    "        lossD_ += d_loss_noassreal\n",
    "        lossD += d_loss_noassreal.item()\n",
    "\n",
    "        fake, gf1 = netg(img)\n",
    "        d_out_fake, df1 = netd(fake.detach())  #需要 detach 因為不希望更新fake的參數\n",
    "        d_loss_fake = th.mean(d_out_fake)\n",
    "\n",
    "        lossD_ += d_loss_fake\n",
    "        lossD += d_loss_fake.item()\n",
    "        gradient_penalty = compute_gradient_penalty(netd, ass_label.data, fake.data)\n",
    "        lossD_ = lossD_/3 + lambda_gp * gradient_penalty\n",
    "#         lossD_ = lossD_/3\n",
    "        lossD_.backward()\n",
    "        optimD.step()\n",
    "        \n",
    "            \n",
    "        # update A\n",
    "        lossA = 0\n",
    "        lossA_ = 0\n",
    "        optimA.zero_grad()\n",
    "        assd = th.cat((img, ass_label), 1)\n",
    "        noassd = th.cat((img, noass_label), 1)\n",
    "        faked, gf1 = netg(img)\n",
    "        faked = th.cat((img, faked.detach()), 1)  #需要 detach 因為不希望更新fake的參數\n",
    "\n",
    "        d_out_assreal,dr1 = neta(assd)\n",
    "        d_loss_assreal = -th.mean(d_out_assreal)\n",
    "        lossA += d_loss_assreal.item()\n",
    "        lossA_ += d_loss_assreal\n",
    "\n",
    "        d_out_noassreal,dr2 = neta(noassd)\n",
    "        d_loss_noassreal = th.mean(d_out_noassreal)\n",
    "\n",
    "        lossA_ += d_loss_noassreal\n",
    "        lossA += d_loss_noassreal.item()\n",
    "        \n",
    "        d_out_faked, df3 = neta(faked)\n",
    "        d_loss_faked = th.mean(d_out_faked)\n",
    "\n",
    "        lossA_ += d_loss_faked\n",
    "        lossA += d_loss_faked.item()\n",
    "        gradient_penalty = compute_gradient_penalty(neta, assd.data, faked.data)\n",
    "        lossA_ = lossA_/3 + lambda_gp * gradient_penalty\n",
    "#         lossA_ = lossA_/3\n",
    "        lossA_.backward()\n",
    "        optimA.step()\n",
    "\n",
    "            \n",
    "        # update G\n",
    "#         if i % n_critic == 0:\n",
    "        lossG = 0\n",
    "        lossG_ = 0\n",
    "        optimG.zero_grad()\n",
    "        fake,_= netg(img)\n",
    "        g_out_fake,_ = netd(fake)\n",
    "        g_loss_fake = - g_out_fake.mean()\n",
    "\n",
    "        lossG += g_loss_fake.item()\n",
    "        lossG_ += g_loss_fake\n",
    "\n",
    "        faked = th.cat((img, fake), 1)\n",
    "        g_out_faked,_ = neta(faked)\n",
    "        g_loss_faked = - g_out_faked.mean()\n",
    "        lossG += g_loss_faked.item()\n",
    "        lossG_ += g_loss_faked\n",
    "        \n",
    "        lossG_ = lossG_/2\n",
    "        lossG_.backward(retain_graph=True) ##其實這裡也不需要retain graph 因為他只 backward 一次；\n",
    "        #如果爾後有要再backward，第一次就需要retain graph\n",
    "        optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}, Gattn={}, Dattn={}, Aattn={}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3, netg.attn.gamma.item(), netd.attn.gamma.item(), neta.attn.gamma.item()\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))           \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA GaitGAN and triplet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T11:04:01.107626Z",
     "start_time": "2019-07-06T04:16:43.132497Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad, Variable\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "\n",
    "epoches = 700\n",
    "glr = 0.00001\n",
    "dlr = 0.00004\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0.9\n",
    "margin = 10\n",
    "n_g = 2\n",
    "n_d = 0\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr, betas=(beta1, beta2))\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "# optimG = optim.RMSprop(netg.parameters(), lr=lr)\n",
    "# optimD = optim.RMSprop(netd.parameters(), lr=lr)\n",
    "# optimA = optim.RMSprop(neta.parameters(), lr=lr)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "#     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    alpha = th.rand((batchSize, 1, 1, 1)).to(device).to(th.float32)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "#     fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = grad(outputs=d_interpolates, \n",
    "                     inputs=interpolates, \n",
    "                     grad_outputs=th.ones([real_samples.shape[0],1]).to(device).requires_grad_(False),\n",
    "#                      grad_outputs = fake,\n",
    "                     create_graph=True, \n",
    "                     retain_graph=True, \n",
    "                     only_inputs=True)[0]\n",
    "#     fake = Variable((real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "#     # Get gradient w.r.t. interpolates\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=d_interpolates,\n",
    "#         inputs=interpolates,\n",
    "#         grad_outputs=fake,\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True,\n",
    "#     )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "#     for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "    for i, (ass_label, noass_label, noass_img, img, ass_img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "    # while epoch < 30000:\n",
    "    #     ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        noass_img = noass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        ass_img = ass_img.to(device).to(th.float32)\n",
    "\n",
    "        if i % n_g ==0:\n",
    "            # update D\n",
    "            lossD = 0\n",
    "            lossD_ = 0\n",
    "            optimD.zero_grad()\n",
    "            d_out_assreal,dr1 = netd(ass_label)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_assreal\n",
    "            lossD += d_loss_assreal.item()\n",
    "\n",
    "            d_out_noassreal,dr2 = netd(noass_label)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 - d_out_noassreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_noassreal\n",
    "            lossD += d_loss_noassreal.item()\n",
    "\n",
    "            fake, code = netg(img)\n",
    "            d_out_fake, df1 = netd(fake.detach())\n",
    "            d_loss_fake = nn.ReLU()(1.0 + d_out_fake).mean()\n",
    "\n",
    "            lossD_ += d_loss_fake\n",
    "            lossD += d_loss_fake.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(netd, ass_label.data, fake.data)\n",
    "            lossD_ = lossD_/3\n",
    "            lossD_.backward()\n",
    "            optimD.step()\n",
    "\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            lossA_ = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            faked, code = netg(img)\n",
    "            faked = th.cat((img, faked.detach()), 1)\n",
    "\n",
    "            d_out_assreal,dr1 = neta(assd)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "            lossA += d_loss_assreal.item()\n",
    "            lossA_ += d_loss_assreal\n",
    "\n",
    "            d_out_noassreal,dr2 = neta(noassd)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 + d_out_noassreal).mean()\n",
    "\n",
    "            lossA_ += d_loss_noassreal\n",
    "            lossA += d_loss_noassreal.item()\n",
    "\n",
    "            d_out_faked, df3 = neta(faked)\n",
    "            d_loss_faked = nn.ReLU()(1.0 + d_out_faked).mean()\n",
    "\n",
    "            lossA_ += d_loss_faked\n",
    "            lossA += d_loss_faked.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(neta, assd.data, faked.data)\n",
    "            lossA_ = lossA_/3\n",
    "            lossA_.backward()\n",
    "            optimA.step()\n",
    "\n",
    "            \n",
    "        # update G\n",
    "        if i % n_d == 0:\n",
    "            lossG = 0\n",
    "            lossG_ = 0\n",
    "            optimG.zero_grad()\n",
    "            fake, A= netg(img)\n",
    "            g_out_fake,_ = netd(fake)\n",
    "            g_loss_fake = - g_out_fake.mean()\n",
    "\n",
    "            lossG += g_loss_fake.item()\n",
    "            lossG_ += g_loss_fake\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            g_out_faked,_ = neta(faked)\n",
    "            g_loss_faked = - g_out_faked.mean()\n",
    "            lossG += g_loss_faked.item()\n",
    "            lossG_ += g_loss_faked\n",
    "\n",
    "            # constrain on generator\n",
    "            fake_ass, P = netg(ass_img)\n",
    "            fake_noass, N = netg(noass_img)\n",
    "            lossTriplet = F.triplet_margin_loss(A, P, N, margin = margin)\n",
    "            lossG_ += lossTriplet\n",
    "            lossG += lossTriplet.item()\n",
    "    #         lossTriplet.backward()\n",
    "\n",
    "            lossG_ = lossG_/3\n",
    "            lossG_.backward(retain_graph=True)\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/3, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/3, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/3, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrTri = {}, ErrA = {}, ErrD = {}, Gattn={}, Dattn={}, Aattn={}'.format(\n",
    "            epoch, lossG/3,lossTriplet.item(), lossA/3, lossD/3, netg.attn.gamma.item(), netd.attn.gamma.item(), neta.attn.gamma.item()\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/3)<low_loss:  \n",
    "        low_loss = lossG/3\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/3, lossA/3, lossD/3\n",
    "        ))           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA GaitGAN and triplet (onlineloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T00:29:55.922173Z",
     "start_time": "2019-07-06T16:40:51.825605Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad, Variable\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00001\n",
    "dlr = 0.00004\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0.9\n",
    "margin = 10\n",
    "n_g = 2\n",
    "n_d = 0\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr, betas=(beta1, beta2))\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "# optimG = optim.RMSprop(netg.parameters(), lr=lr)\n",
    "# optimD = optim.RMSprop(netd.parameters(), lr=lr)\n",
    "# optimA = optim.RMSprop(neta.parameters(), lr=lr)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "#     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    alpha = th.rand((batchSize, 1, 1, 1)).to(device).to(th.float32)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "#     fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = grad(outputs=d_interpolates, \n",
    "                     inputs=interpolates, \n",
    "                     grad_outputs=th.ones([real_samples.shape[0],1]).to(device).requires_grad_(False),\n",
    "#                      grad_outputs = fake,\n",
    "                     create_graph=True, \n",
    "                     retain_graph=True, \n",
    "                     only_inputs=True)[0]\n",
    "#     fake = Variable((real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "#     # Get gradient w.r.t. interpolates\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=d_interpolates,\n",
    "#         inputs=interpolates,\n",
    "#         grad_outputs=fake,\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True,\n",
    "#     )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "#     for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "    for i, (ass_label, noass_label, noass_img, img, ass_img, label_neg, label_anc, label_pos) in enumerate(train_loader):\n",
    "        \n",
    "        com_img = th.cat((noass_img, img, ass_img), 0)\n",
    "        com_label = th.cat(( label_neg, label_anc, label_pos), 0)\n",
    "        com_img = com_img.to(device).to(th.float32)\n",
    "        com_label = com_label.to(device).to(th.float32)\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        noass_img = noass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        ass_img = ass_img.to(device).to(th.float32)\n",
    "\n",
    "        if i % n_g ==0:\n",
    "            # update D\n",
    "            lossD = 0\n",
    "            lossD_ = 0\n",
    "            optimD.zero_grad()\n",
    "            d_out_assreal,dr1 = netd(ass_label)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_assreal\n",
    "            lossD += d_loss_assreal.item()\n",
    "\n",
    "            d_out_noassreal,dr2 = netd(noass_label)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 - d_out_noassreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_noassreal\n",
    "            lossD += d_loss_noassreal.item()\n",
    "\n",
    "            fake, code = netg(img)\n",
    "            d_out_fake, df1 = netd(fake.detach())\n",
    "            d_loss_fake = nn.ReLU()(1.0 + d_out_fake).mean()\n",
    "\n",
    "            lossD_ += d_loss_fake\n",
    "            lossD += d_loss_fake.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(netd, ass_label.data, fake.data)\n",
    "            lossD_ = lossD_/3\n",
    "            lossD_.backward()\n",
    "            optimD.step()\n",
    "\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            lossA_ = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            faked, code = netg(img)\n",
    "            faked = th.cat((img, faked.detach()), 1)\n",
    "\n",
    "            d_out_assreal,dr1 = neta(assd)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "            lossA += d_loss_assreal.item()\n",
    "            lossA_ += d_loss_assreal\n",
    "\n",
    "            d_out_noassreal,dr2 = neta(noassd)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 + d_out_noassreal).mean()\n",
    "\n",
    "            lossA_ += d_loss_noassreal\n",
    "            lossA += d_loss_noassreal.item()\n",
    "\n",
    "            d_out_faked, df3 = neta(faked)\n",
    "            d_loss_faked = nn.ReLU()(1.0 + d_out_faked).mean()\n",
    "\n",
    "            lossA_ += d_loss_faked\n",
    "            lossA += d_loss_faked.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(neta, assd.data, faked.data)\n",
    "            lossA_ = lossA_/3\n",
    "            lossA_.backward()\n",
    "            optimA.step()\n",
    "\n",
    "\n",
    "            \n",
    "        # update G\n",
    "        if i % n_d == 0:\n",
    "            lossG = 0\n",
    "            lossG_ = 0\n",
    "            optimG.zero_grad()\n",
    "            fake, A= netg(img)\n",
    "            g_out_fake,_ = netd(fake)\n",
    "            g_loss_fake = - g_out_fake.mean()\n",
    "\n",
    "            lossG += g_loss_fake.item()\n",
    "            lossG_ += g_loss_fake\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            g_out_faked,_ = neta(faked)\n",
    "            g_loss_faked = - g_out_faked.mean()\n",
    "            lossG += g_loss_faked.item()\n",
    "            lossG_ += g_loss_faked\n",
    "\n",
    "    #         # constrain on encoder\n",
    "    #         fake_ass, P = netg(ass_img)\n",
    "    #         fake_noass, N = netg(noass_img)\n",
    "    #         lossTriplet = F.triplet_margin_loss(A, P, N, margin = margin)\n",
    "    #         lossG_ += lossTriplet\n",
    "    #         lossG += lossTriplet.item()\n",
    "    # #         lossTriplet.backward()\n",
    "\n",
    "            ## new onlinetripletloss\n",
    "            __, com = netg(com_img)\n",
    "            loss_fn = OnlineTripletLoss(margin, SemihardNegativeTripletSelector(margin))\n",
    "            lossTriplet,len_triplet = loss_fn(com, com_label)\n",
    "            lossG += lossTriplet.item()\n",
    "            lossG_ += lossTriplet\n",
    "    #         lossTriplet.backward()\n",
    "\n",
    "\n",
    "            lossG_ = lossG_/3\n",
    "            lossG_.backward(retain_graph=True)\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/3, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/3, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/3, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrTri = {}, ErrA = {}, ErrD = {}, Gattn={}, Dattn={}, Aattn={}'.format(\n",
    "            epoch, lossG/3,lossTriplet.item(), lossA/3, lossD/3, netg.attn.gamma.item(), netd.attn.gamma.item(), neta.attn.gamma.item()\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/3)<low_loss:  \n",
    "        low_loss = lossG/3\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/3, lossA/3, lossD/3\n",
    "        ))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36_ting_cv]",
   "language": "python",
   "name": "conda-env-py36_ting_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
