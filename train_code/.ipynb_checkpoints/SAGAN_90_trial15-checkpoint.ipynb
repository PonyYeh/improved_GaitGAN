{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T01:43:40.025007Z",
     "start_time": "2019-08-13T01:43:39.415275Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# speed up the loading of the training data\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from onlineTripletloss import *\n",
    "from selector import *\n",
    "from model_SAGAN1_2 import NetG, NetD, NetA\n",
    "# from model_SAGAN2_Triplet import NetG, NetD, NetA\n",
    "# from model_WGANGP import NetG, NetD, NetA\n",
    "# from model_WGAN import NetG, NetD, NetA\n",
    "# from model_siGAN import NetG, NetD, NetA\n",
    "from dataset2Loader import CASIABDataset\n",
    "# from dataset2Loader_newtriplet import CASIABDataset\n",
    "# from dataset2Loader_triplet import CASIABDataset\n",
    "import torch.optim as optim\n",
    "import visdom\n",
    "from torchvision.utils import make_grid\n",
    "# Data_Dir = '../GaitRecognition/DatasetB_GEI_64x64_allseq/'\n",
    "Data_Dir = '../GaitRecognition/GEI_CASIA_B/gei/'\n",
    "Model_Name = 'Model_64x64_SAGAN_90_trial12'\n",
    "Model_dir = './Transform_Model/'+ Model_Name\n",
    "if not os.path.isdir(Model_dir):\n",
    "    os.mkdir(Model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:51:23.878764Z",
     "start_time": "2019-05-17T11:51:22.490886Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Data_Dir = '../GaitRecognition/GEI_CASIA_B/gei/'\n",
    "import os\n",
    "\n",
    "for r, d, files in os.walk(Data_Dir):\n",
    "    print(r)\n",
    "    print(len(d))\n",
    "    print(len(files))\n",
    "    \n",
    "# cpt = sum([len(files) for r, d, files in os.walk(Data_Dir)])\n",
    "# cpt = sum([len(d) for r, d, files in os.walk(Data_Dir)])\n",
    "# print(cpt)\n",
    "# list = os.listdir(Data_Dir) # dir is your directory path\n",
    "# number_files = len(list)\n",
    "# print(number_files)\n",
    "\n",
    "# import fnmatch\n",
    "# print(len(fnmatch.filter(os.listdir(Data_Dir), '*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T06:02:12.747164Z",
     "start_time": "2019-05-27T06:02:12.676816Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = CASIABDataset(data_dir=Data_Dir)\n",
    "ass_label, noass_label, img = dataset.getbatch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iteration 刪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T08:08:38.785029Z",
     "start_time": "2019-06-16T08:08:38.768214Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(tensor, gain=1.)\n",
    "#         init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir)\n",
    "\n",
    "iteration = 0\n",
    "lr = 0.00002\n",
    "# lr = 0.0002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 128\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=lr/2)\n",
    "optimD = optim.Adam(netd.parameters(), lr=lr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=lr/3)\n",
    "\n",
    "print('Training starts')\n",
    "while iteration < 30000:\n",
    "    ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "    ass_label = ass_label.to(device).to(th.float32)\n",
    "    noass_label = noass_label.to(device).to(th.float32)\n",
    "    img = img.to(device).to(th.float32)\n",
    "    \n",
    "    # update D\n",
    "    lossD = 0\n",
    "    optimD.zero_grad()\n",
    "    output = netd(ass_label)\n",
    "    label.fill_(real_label)\n",
    "    lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "    lossD += lossD_real1.item()\n",
    "    lossD_real1.backward()\n",
    "\n",
    "    label.fill_(real_label)\n",
    "    output1 = netd(noass_label)\n",
    "    lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "    lossD += lossD_real2.item()\n",
    "    lossD_real2.backward()\n",
    "\n",
    "    fake = netg(img).detach()\n",
    "    label.fill_(fake_label)\n",
    "    output2 = netd(fake)\n",
    "    lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "    lossD += lossD_fake.item()\n",
    "    lossD_fake.backward()\n",
    "\n",
    "    optimD.step()\n",
    "    \n",
    "    # update A\n",
    "    lossA = 0\n",
    "    optimA.zero_grad()\n",
    "    assd = th.cat((img, ass_label), 1)\n",
    "    noassd = th.cat((img, noass_label), 1)\n",
    "    fake = netg(img).detach()\n",
    "    faked = th.cat((img, fake), 1)\n",
    "\n",
    "    label.fill_(real_label)\n",
    "    output1 = neta(assd)\n",
    "    lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "    lossA += lossA_real1.item()\n",
    "    lossA_real1.backward()\n",
    "\n",
    "    label.fill_(fake_label)\n",
    "    output = neta(noassd)\n",
    "    lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "    lossA += lossA_real2.item()\n",
    "    lossA_real2.backward()\n",
    "\n",
    "    label.fill_(fake_label)\n",
    "    output = neta(faked)\n",
    "    lossA_fake = F.binary_cross_entropy(output, label)\n",
    "    lossA += lossA_fake.item()\n",
    "    lossA_fake.backward()\n",
    "    \n",
    "    optimA.step()\n",
    "    \n",
    "    # update G\n",
    "    lossG = 0\n",
    "    optimG.zero_grad()\n",
    "    fake = netg(img)\n",
    "    output = netd(fake)\n",
    "\n",
    "    label.fill_(real_label)\n",
    "    lossGD = F.binary_cross_entropy(output, label)\n",
    "    lossG += lossGD.item()\n",
    "    lossGD.backward(retain_graph=True)\n",
    "\n",
    "    faked = th.cat((img, fake), 1)\n",
    "    output = neta(faked)\n",
    "    label.fill_(real_label)\n",
    "    lossGA = F.binary_cross_entropy(output, label)\n",
    "    lossG += lossGA.item()\n",
    "    lossGA.backward()\n",
    "    \n",
    "    optimG.step()\n",
    "\n",
    "    iteration += 1\n",
    "    \n",
    "    if iteration % 20 == 0:\n",
    "        with th.no_grad():\n",
    "            netg.eval()  #切換\n",
    "            fake = netg(img) 25\n",
    "            netg.train() #切換回去\n",
    "        fake = (fake + 1) / 2 * 255\n",
    "        real = (ass_label + 1) / 2 * 255\n",
    "        ori = (img + 1) / 2 * 255\n",
    "        al = th.cat((fake, real, ori), 2)\n",
    "        display = make_grid(al, 20).cpu().numpy()\n",
    "        if win1 is None:\n",
    "            win1 = vis.image(display,\n",
    "                             opts=dict(title=\"train\", caption='train'))\n",
    "        else:\n",
    "            vis.image(display, win=win1)\n",
    "    if iteration % 1000 == 0 or iteration==500:\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_Name+'/snapshot'+ Model_Name +'_%d.t7' % iteration)\n",
    "\n",
    "    if iteration % 100==0 or iteration==10 :\n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[iteration, iteration,\n",
    "                                        iteration]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='iterations',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[iteration, iteration,\n",
    "                                  iteration]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "            \n",
    "        with open(Model_Name+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('iter = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            iteration, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('iter = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            iteration, lossG/2, lossA/3, lossD/3\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T05:25:59.661560Z",
     "start_time": "2019-05-27T05:25:59.620596Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = CASIABDataset(data_dir=Data_Dir)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=False)\n",
    "train_loader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T08:46:23.327475Z",
     "start_time": "2019-06-16T08:23:19.736325Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "\n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0\n",
    "margin = 0\n",
    "n_g = 0\n",
    "n_d = 0\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr/2)\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "print('Training starts')\n",
    "low_loss = 10\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#     for i, (ass_label, noass_label, noass_img, img, ass_img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "#         ass_img = ass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "#         noass_img = noass_img.to(device).to(th.float32)\n",
    "\n",
    "        if i % n_g ==0:\n",
    "            # update D\n",
    "            lossD = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "            label.fill_(real_label)\n",
    "            lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD += lossD_real1.item()\n",
    "            lossD_real1.backward()\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "            lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD += lossD_real2.item()\n",
    "            lossD_real2.backward()\n",
    "\n",
    "            fake = netg(img).detach()\n",
    "            label.fill_(fake_label)\n",
    "            output2 = netd(fake)\n",
    "            lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD += lossD_fake.item()\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            fake = netg(img).detach()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "            lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "            lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "            lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "\n",
    "        if i % n_d ==0: \n",
    "            # update G\n",
    "            lossG = 0\n",
    "            optimG.zero_grad()\n",
    "            fake = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGD.item()\n",
    "            lossGD.backward(retain_graph=True)\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "            label.fill_(real_label)\n",
    "            lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGA.item()\n",
    "            lossGA.backward()\n",
    "\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    \n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:  \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "    \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update k times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T12:08:32.658055Z",
     "start_time": "2019-06-15T08:18:03.792818Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0\n",
    "margin = 0\n",
    "n_g = 2\n",
    "n_d = 0\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr/2)\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#     for i, (ass_label, noass_label, noass_img, img, ass_img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "#         noass_img = noass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "#         ass_img = ass_img.to(device).to(th.float32)\n",
    "        \n",
    "        if i % n_g ==0:\n",
    "            # update D\n",
    "            lossD = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "            label.fill_(real_label)\n",
    "            lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD += lossD_real1.item()\n",
    "            lossD_real1.backward()\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "            lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD += lossD_real2.item()\n",
    "            lossD_real2.backward()\n",
    "\n",
    "            fake = netg(img).detach()\n",
    "            label.fill_(fake_label)\n",
    "            output2 = netd(fake)\n",
    "            lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD += lossD_fake.item()\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake = netg(img).detach()\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "            lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "            lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "            lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "        \n",
    "        if i % n_d ==0: \n",
    "            # update G\n",
    "            lossG = 0\n",
    "            optimG.zero_grad()\n",
    "            fake = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGD.item()\n",
    "            lossGD.backward(retain_graph=True)\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "            label.fill_(real_label)\n",
    "            lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGA.item()\n",
    "            lossGA.backward()\n",
    "\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    \n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model changed +Dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T09:39:59.973452Z",
     "start_time": "2019-06-17T01:57:16.530081Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netp = NetP(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netp.children())[0].children(),\n",
    "    list(neta.children())[0].children(),\n",
    "    list(netd.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "epoches = 700\n",
    "lr = 0.00002\n",
    "# lr = 0.0002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "\n",
    "netg = netg.to(device)\n",
    "netp = netp.to(device)\n",
    "neta = neta.to(device)\n",
    "netd = netd.to(device)\n",
    "netg.train()\n",
    "netp.train()\n",
    "neta.train()\n",
    "netd.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=lr/2)\n",
    "optimP = optim.Adam(netp.parameters(), lr=lr/3)\n",
    "optimD = optim.Adam(netd.parameters(), lr=lr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=lr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, lr = {}, batchsize = {}, target = {} \\n'.format(\n",
    "            epoches, lr, batchSize, target))\n",
    "\n",
    "print('Training starts')\n",
    "low_loss = 10\n",
    "for epoch in range(1,epoches+1):\n",
    "#     for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "    for i, (ass_label, noass_label, ass_img, img, noass_img) in enumerate(train_loader): \n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "    # while epoch < 30000:\n",
    "    #     ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        ass_img = ass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        noass_img = noass_img.to(device).to(th.float32)\n",
    "        \n",
    "        # update D\n",
    "        lossD = 0\n",
    "        optimD.zero_grad()\n",
    "        output = netd(ass_label)\n",
    "        label.fill_(real_label)\n",
    "        lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "        lossD += lossD_real1.item()\n",
    "        lossD_real1.backward()\n",
    "\n",
    "        label.fill_(real_label)\n",
    "        output1 = netd(noass_label)\n",
    "        lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "        lossD += lossD_real2.item()\n",
    "        lossD_real2.backward()\n",
    "\n",
    "        fake = netg(img).detach()\n",
    "        label.fill_(fake_label)\n",
    "        output2 = netd(fake)\n",
    "        lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "        lossD += lossD_fake.item()\n",
    "        lossD_fake.backward()\n",
    "\n",
    "        optimD.step()\n",
    "\n",
    "\n",
    "        # update P\n",
    "        lossP = 0\n",
    "        optimP.zero_grad()\n",
    "        fake = netg(img).detach()\n",
    "        assd = th.cat((img, ass_img), 1)\n",
    "        noassd = th.cat((img, noass_img), 1)\n",
    "        faked = th.cat((img, fake), 1)\n",
    "\n",
    "        label.fill_(real_label)\n",
    "        output1 = neta(assd)\n",
    "        lossP_real1 = F.binary_cross_entropy(output1, label)\n",
    "        lossP += lossP_real1.item()/2\n",
    "        lossP_real1 = lossP_real1/2\n",
    "        lossP_real1.backward()\n",
    "\n",
    "        label.fill_(fake_label)\n",
    "        output = neta(noassd)\n",
    "        lossP_real2 = F.binary_cross_entropy(output, label)\n",
    "        lossP += lossP_real2.item()/2\n",
    "        lossP_real2 = lossP_real2/2\n",
    "        lossP_real2.backward()\n",
    "\n",
    "        label.fill_(fake_label)\n",
    "        output = neta(faked)\n",
    "        lossP_fake = F.binary_cross_entropy(output, label)\n",
    "        lossP += lossP_fake.item()/2\n",
    "        lossP_fake = lossP_fake/2\n",
    "        lossP_fake.backward()\n",
    "\n",
    "        optimP.step()\n",
    "    \n",
    "\n",
    "        # update A\n",
    "        lossA = 0\n",
    "        optimA.zero_grad()\n",
    "        fake = netg(img).detach()\n",
    "        assd = th.cat((img, ass_label), 1)\n",
    "        noassd = th.cat((img, noass_label), 1)\n",
    "        faked = th.cat((img, fake), 1)\n",
    "\n",
    "        label.fill_(real_label)\n",
    "        output1 = neta(assd)\n",
    "        lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "        lossA += lossA_real1.item()\n",
    "        lossA_real1.backward()\n",
    "\n",
    "        label.fill_(fake_label)\n",
    "        output = neta(noassd)\n",
    "        lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "        lossA += lossA_real2.item()\n",
    "        lossA_real2.backward()\n",
    "\n",
    "        label.fill_(fake_label)\n",
    "        output = neta(faked)\n",
    "        lossA_fake = F.binary_cross_entropy(output, label)\n",
    "        lossA += lossA_fake.item()\n",
    "        lossA_fake.backward()\n",
    "\n",
    "        optimA.step()\n",
    "\n",
    "        # update G\n",
    "        lossG = 0\n",
    "        optimG.zero_grad()\n",
    "        \n",
    "        fake = netg(img)\n",
    "        output = netd(fake)\n",
    "        label.fill_(real_label)\n",
    "        lossGD = F.binary_cross_entropy(output, label)\n",
    "        lossG += lossGD.item()\n",
    "        lossGD.backward(retain_graph=True)\n",
    "\n",
    "        faked = th.cat((img, fake), 1)\n",
    "        output = neta(faked)\n",
    "        label.fill_(real_label)\n",
    "        lossGA = F.binary_cross_entropy(output, label)\n",
    "        lossG += lossGA.item()\n",
    "        lossGA.backward(retain_graph=True)\n",
    "        \n",
    "        faked = th.cat((img, fake), 1)\n",
    "        output = netp(faked)\n",
    "        label.fill_(real_label)\n",
    "        lossGP = F.binary_cross_entropy(output, label)\n",
    "        lossG += lossGP.item()/2\n",
    "        lossGP = lossGP/2\n",
    "        lossGP.backward()\n",
    "\n",
    "        optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    \n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch, epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3, lossP/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD', 'lossP']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch, epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3, lossP/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}, ErrP = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3, lossP/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}, ErrP = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3, lossP/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:  \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict(),\n",
    "            'netP': netp.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "    \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict(),\n",
    "            'netP': netp.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}, ErrP = {}  \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3, lossP/3\n",
    "        ))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model only D_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T05:21:59.261901Z",
     "start_time": "2019-06-23T02:19:01.856865Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "epoches = 700\n",
    "lr = 0.00002\n",
    "# lr = 0.0002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "g_k = 2\n",
    "target = '090'\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "# netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=lr/2)\n",
    "# optimD = optim.Adam(netd.parameters(), lr=lr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=lr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, lr = {}, batchsize = {}, g_k = {}, target={} \\n'.format(\n",
    "            epoches, lr, batchSize, g_k, target))\n",
    "\n",
    "low_loss = 10\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#     for i, (ass_label, noass_label, noass_img, img, ass_img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "    # while epoch < 30000:\n",
    "    #     ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "#         noass_img = noass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "#         ass_img = ass_img.to(device).to(th.float32)\n",
    "        \n",
    "        if i % g_k ==0:\n",
    "#             # update D\n",
    "#             lossD = 0\n",
    "#             optimD.zero_grad()\n",
    "#             output = netd(ass_label)\n",
    "#             label.fill_(real_label)\n",
    "#             lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "#             lossD += lossD_real1.item()\n",
    "#             lossD_real1.backward()\n",
    "\n",
    "#             label.fill_(real_label)\n",
    "#             output1 = netd(noass_label)\n",
    "#             lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "#             lossD += lossD_real2.item()\n",
    "#             lossD_real2.backward()\n",
    "\n",
    "#             fake = netg(img).detach()\n",
    "#             label.fill_(fake_label)\n",
    "#             output2 = netd(fake)\n",
    "#             lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "#             lossD += lossD_fake.item()\n",
    "#             lossD_fake.backward()\n",
    "\n",
    "#             optimD.step()\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake = netg(img).detach()\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "            lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "            lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "            lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "        \n",
    "#         if i % k ==0: \n",
    "        # update G\n",
    "        lossG = 0\n",
    "        optimG.zero_grad()\n",
    "        fake = netg(img)\n",
    "        output = netd(fake)\n",
    "\n",
    "        label.fill_(real_label)\n",
    "        lossGD = F.binary_cross_entropy(output, label)\n",
    "        lossG += lossGD.item()\n",
    "        lossGD.backward(retain_graph=True)\n",
    "\n",
    "        faked = th.cat((img, fake), 1)\n",
    "        output = neta(faked)\n",
    "        label.fill_(real_label)\n",
    "        lossGA = F.binary_cross_entropy(output, label)\n",
    "        lossG += lossGA.item()\n",
    "        lossGA.backward()\n",
    "\n",
    "        optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    \n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        ]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  ]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3,\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, \n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}\\n'.format(\n",
    "            epoch, lossG/2, lossA/3\n",
    "        ))    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaitGAN and triplet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T21:13:30.999315Z",
     "start_time": "2019-05-29T14:16:28.502269Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '036'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0\n",
    "margin = 5\n",
    "n_g = 0\n",
    "n_d = 0\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr/2)\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "\n",
    "        # update D\n",
    "        if i % n_g==0:\n",
    "            lossD = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "            label.fill_(real_label)\n",
    "            lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD += lossD_real1.item()\n",
    "            lossD_real1.backward()\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "            lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD += lossD_real2.item()\n",
    "            lossD_real2.backward()\n",
    "\n",
    "            fake, _ = netg(img)\n",
    "            label.fill_(fake_label)\n",
    "            output2 = netd(fake.detach())\n",
    "            lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD += lossD_fake.item()\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake,_ = netg(img)\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "            lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "            lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(faked.detach())\n",
    "            lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "        \n",
    "        if i % n_d ==0: \n",
    "            # update G\n",
    "            lossG = 0\n",
    "            optimG.zero_grad()\n",
    "            fake, A = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGD.item()\n",
    "            lossGD.backward(retain_graph=True)\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "            label.fill_(real_label)\n",
    "            lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGA.item()\n",
    "            lossGA.backward()\n",
    "\n",
    "            # constrain on generator\n",
    "            fake_ass, P = netg(ass_label)\n",
    "            fake_noass, N = netg(noass_label)\n",
    "            lossTriplet = F.triplet_margin_loss(fake, fake_ass, fake_noass, margin = margin)\n",
    "            lossG += lossTriplet.item()\n",
    "            lossTriplet.backward()\n",
    "\n",
    "            # constrain on encoder\n",
    "    #         __, P = netg(ass_label)\n",
    "    #         __, N = netg(noass_label)\n",
    "    #         lossTriplet = F.triplet_margin_loss(A, P, N, margin = margin)\n",
    "    #         lossG += lossTriplet.item()\n",
    "    #         lossTriplet.backward()\n",
    "\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaitGAN k times and triplet   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T17:27:36.724552Z",
     "start_time": "2019-06-18T06:30:54.000065Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0\n",
    "margin = 10\n",
    "n_g = 2\n",
    "n_d = 2\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr/3)\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr/3)\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, noass_img, img, ass_img) in enumerate(train_loader):\n",
    "#         com_img = th.cat((noass_img, img, ass_img), 0)\n",
    "#         com_label = th.cat(( label_neg, label_anc, label_pos), 0)\n",
    "#         com_img = com_img.to(device).to(th.float32)\n",
    "#         com_label = com_label.to(device).to(th.float32)\n",
    "        \n",
    "#         if(i ==0):\n",
    "#             print(label_neg,label_anc,label_pos)\n",
    "#             print(com_label)\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape,com_img.shape, com_label.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        noass_img = noass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        ass_img = ass_img.to(device).to(th.float32)\n",
    "        \n",
    "        # update D\n",
    "        if i % n_g==0:\n",
    "            lossD = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "            label.fill_(real_label)\n",
    "            lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD += lossD_real1.item()\n",
    "            lossD_real1.backward()\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "            lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD += lossD_real2.item()\n",
    "            lossD_real2.backward()\n",
    "\n",
    "            fake, _ = netg(img)\n",
    "            label.fill_(fake_label)\n",
    "            output2 = netd(fake.detach()) #需要 detach 因為不希望更新fake的參數\n",
    "            lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD += lossD_fake.item()\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake,_ = netg(img)\n",
    "            faked = th.cat((img, fake.detach()), 1)  #需要 detach 因為不希望更新fake的參數\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "            lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "            lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "            label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "            lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "        \n",
    "        if i % n_d ==0: \n",
    "            # update G\n",
    "            lossG = 0\n",
    "            optimG.zero_grad()\n",
    "            fake, A = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "            label.fill_(real_label)\n",
    "            lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGD.item()\n",
    "            lossGD.backward(retain_graph=True)  ##這裡需要retain graph 因為他之後有需要fake，因此需要retain\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "            label.fill_(real_label)\n",
    "            lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossG += lossGA.item()\n",
    "            lossGA.backward()\n",
    "\n",
    "            ## new tripletloss\n",
    "            _, P = netg(ass_img)\n",
    "            __, N = netg(noass_img)\n",
    "            lossf = TripletLoss(margin) #因為之前的A都沒被使用過所以不會遺失，如果是拿fake ，LossGA就需要再retain graph\n",
    "            lossTriplet =lossf(A, P, N)\n",
    "            lossG += lossTriplet.item()\n",
    "            lossTriplet.backward()\n",
    "\n",
    "    #         ## new onlinetripletloss\n",
    "    #         __, com = netg(com_img)\n",
    "    #         loss_fn = OnlineTripletLoss(margin, SemihardNegativeTripletSelector(margin))\n",
    "    #         lossTriplet,len_triplet = loss_fn(com, com_label)\n",
    "    #         lossG += lossTriplet.item()\n",
    "    #         lossTriplet.backward()\n",
    "    # #         print(lossTriplet.item(),len_triplet)\n",
    "\n",
    "    #         ## triplet loss\n",
    "    #         __, P = netg(ass_img)\n",
    "    #         __, N = netg(noass_img)\n",
    "    #         lossTriplet = F.triplet_margin_loss(A, P, N, margin = margin) #因為之前的A都沒被使用過所以不會遺失，如果是拿fake ，LossGA就需要再retain graph\n",
    "    #         lossG += lossTriplet.item()\n",
    "    #         lossTriplet.backward()\n",
    "    # #         if i%10==0:\n",
    "    # #             print(\"tripletloss \",lossTriplet.item())\n",
    "\n",
    "            ## tripletloss no negative\n",
    "    #         N_plus = th.zeros((A.size()), requires_grad=False).to(device)\n",
    "    #         lossTriplet_AP = F.triplet_margin_loss(A, P, N_plus, margin = margin)\n",
    "    #         lossG += lossTriplet_AP.item()\n",
    "    #         lossTriplet += lossTriplet_AP\n",
    "    #         lossTriplet.backward()\n",
    "\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG  Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from PixelDT code刪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-13T15:13:36.331308Z",
     "start_time": "2019-04-13T08:01:32.315792Z"
    }
   },
   "outputs": [],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir)\n",
    "\n",
    "iteration = 0\n",
    "lr = 0.0002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 128\n",
    "\n",
    "real_label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "fake_label = th.ones((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.Adam(netg.parameters(), lr=lr)\n",
    "optimD = optim.Adam(netd.parameters(), lr=lr)\n",
    "optimA = optim.Adam(neta.parameters(), lr=lr)\n",
    "\n",
    "print('Training starts')\n",
    "while iteration < 40000:\n",
    "    ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "    ass_label = ass_label.to(device).to(th.float32)\n",
    "    noass_label = noass_label.to(device).to(th.float32)\n",
    "    img = img.to(device).to(th.float32)\n",
    "    \n",
    "    # update D\n",
    "    lossD = 0\n",
    "    optimD.zero_grad()\n",
    "    output = netd(ass_label)\n",
    "#     label.fill_(real_label)\n",
    "    lossD_real1 = F.binary_cross_entropy(output, real_label)\n",
    "#     lossD += lossD_real1.item()\n",
    "#     lossD_real1.backward()\n",
    "\n",
    "#     label.fill_(real_label)\n",
    "    output1 = netd(noass_label)\n",
    "    lossD_real2 = F.binary_cross_entropy(output1, real_label)\n",
    "#     lossD += lossD_real2.item()\n",
    "#     lossD_real2.backward()\n",
    "\n",
    "    fake = netg(img).detach()\n",
    "#     label.fill_(fake_label)\n",
    "    output2 = netd(fake)\n",
    "    lossD_fake = F.binary_cross_entropy(output2, fake_label)\n",
    "#     lossD += lossD_fake.item()\n",
    "#     lossD_fake.backward()\n",
    "    lossD = (lossD_real1+ lossD_real2+ lossD_fake)/3\n",
    "    lossD.backward()\n",
    "\n",
    "    lossD_item = lossD.item()\n",
    "    optimD.step()\n",
    "    \n",
    "    # update A\n",
    "    lossA = 0\n",
    "    optimA.zero_grad()\n",
    "    assd = th.cat((img, ass_label), 1)\n",
    "    noassd = th.cat((img, noass_label), 1)\n",
    "    fake = netg(img).detach()\n",
    "    faked = th.cat((img, fake), 1)\n",
    "\n",
    "#     label.fill_(real_label)\n",
    "    output = neta(assd)\n",
    "    lossA_real1 = F.binary_cross_entropy(output, real_label)\n",
    "#     lossA += lossA_real1.item()\n",
    "#     lossA_real1.backward()\n",
    "\n",
    "#     label.fill_(fake_label)\n",
    "    output1 = neta(noassd)\n",
    "    lossA_real2 = F.binary_cross_entropy(output1, fake_label)\n",
    "#     lossA += lossA_real2.item()\n",
    "#     lossA_real2.backward()\n",
    "\n",
    "#     label.fill_(fake_label)\n",
    "    output2 = neta(faked)\n",
    "    lossA_fake = F.binary_cross_entropy(output2, fake_label)\n",
    "#     lossA += lossA_fake.item()\n",
    "#     lossA_fake.backward()\n",
    "    lossA = (lossA_real1+ lossA_real2 +lossA_fake)/3\n",
    "    lossA.backward()\n",
    "    \n",
    "    lossA_item = lossA.item()\n",
    "    optimA.step()\n",
    "    \n",
    "    # update G\n",
    "    lossG = 0\n",
    "    optimG.zero_grad()\n",
    "    fake = netg(img)\n",
    "    output = netd(fake)\n",
    "#     label.fill_(real_label)\n",
    "    lossGD = F.binary_cross_entropy(output, real_label)\n",
    "#     lossG += lossGD.item()\n",
    "#     lossGD.backward(retain_graph=True)\n",
    "\n",
    "    faked = th.cat((img, fake), 1)\n",
    "    output = neta(faked)\n",
    "#     label.fill_(real_label)\n",
    "    lossGA = F.binary_cross_entropy(output, real_label)\n",
    "#     lossG += lossGA.item()\n",
    "#     lossGA.backward()\n",
    "    lossG = (lossGD + lossGA)/2\n",
    "    lossG.backward()\n",
    "    \n",
    "    lossG_item = lossG.item()\n",
    "    optimG.step()\n",
    "\n",
    "    iteration += 1\n",
    "    \n",
    "    if iteration % 20 == 0:\n",
    "        with th.no_grad():\n",
    "            netg.eval()  #切換\n",
    "            fake = netg(img) \n",
    "            netg.train() #切換回去\n",
    "        fake = (fake + 1) / 2 * 255\n",
    "        real = (ass_label + 1) / 2 * 255\n",
    "        ori = (img + 1) / 2 * 255\n",
    "        al = th.cat((fake, real, ori), 2)\n",
    "        display = make_grid(al, 20).cpu().numpy()\n",
    "        if win1 is None:\n",
    "            win1 = vis.image(display,\n",
    "                             opts=dict(title=\"train\", caption='train'))\n",
    "        else:\n",
    "            vis.image(display, win=win1)\n",
    "    if iteration % 5000 == 0 or iteration==500:\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_Name+'/snapshot'+ Model_Name +'_%d.t7' % iteration)\n",
    "        with open(Model_Name+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('iter = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            iteration, lossG_item, lossA_item, lossD_item\n",
    "        ))\n",
    "        print('iter = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            iteration, lossG_item, lossA_item, lossD_item\n",
    "        ))\n",
    "    if iteration % 5000==0 or iteration==10 or iteration==500:\n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[iteration, iteration,\n",
    "                                        iteration]]),\n",
    "                           Y=np.array([[lossG_item, lossA_item, lossD_item]]),\n",
    "                           opts=dict(\n",
    "                               title='GaitGAN',\n",
    "                               ylabel='loss',\n",
    "                               xlabel='iterations',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[iteration, iteration,\n",
    "                                  iteration]]),\n",
    "                     Y=np.array([[lossG_item, lossA_item, lossD_item]]),\n",
    "                     win=win,\n",
    "                     update='append')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-12T16:51:42.619Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_con= 10 ,n_ang= 11\n",
      "target =  090\n",
      "write parameter log...\n",
      "Training starts\n",
      "Epoch = 2, ErrG = 3.752237558364868, ErrA = -1.8985527356465657, ErrD = -3.5465147892634072\n",
      "Epoch = 4, ErrG = 5.705430269241333, ErrA = -2.807316621144613, ErrD = -5.611066023508708\n",
      "Epoch = 6, ErrG = 4.896071195602417, ErrA = -3.07237180074056, ErrD = -6.601948658625285\n",
      "Epoch = 8, ErrG = 8.87260103225708, ErrA = -4.403616587320964, ErrD = -9.87374464670817\n",
      "Epoch = 10, ErrG = 7.947443723678589, ErrA = -4.1470723152160645, ErrD = -8.064921021461487\n",
      "Epoch = 12, ErrG = 1.238088607788086, ErrA = -4.921435038248698, ErrD = -4.710442225138347\n",
      "Epoch = 14, ErrG = 0.5194168090820312, ErrA = -5.6255998611450195, ErrD = -5.323735237121582\n",
      "Epoch = 16, ErrG = 6.348399639129639, ErrA = -7.5277970631917315, ErrD = -7.016017913818359\n",
      "Epoch = 18, ErrG = 0.8842306137084961, ErrA = -7.25491460164388, ErrD = -7.062686284383138\n",
      "Epoch = 20, ErrG = 0.0035724639892578125, ErrA = -7.803771336873372, ErrD = -7.927731831868489\n",
      "Epoch = 22, ErrG = 1.2339839935302734, ErrA = -8.73021380106608, ErrD = -8.565280278523764\n",
      "Epoch = 24, ErrG = 1.2255611419677734, ErrA = -9.72688357035319, ErrD = -9.670048395792643\n",
      "Epoch = 26, ErrG = 0.15535354614257812, ErrA = -10.620674769083658, ErrD = -10.38158925374349\n",
      "Epoch = 28, ErrG = 4.176642417907715, ErrA = -12.133074442545572, ErrD = -10.76137606302897\n",
      "Epoch = 30, ErrG = 4.427227973937988, ErrA = -12.199844360351562, ErrD = -12.158490498860678\n",
      "Epoch = 32, ErrG = 3.9799652099609375, ErrA = -14.347677866617838, ErrD = -14.839614232381185\n",
      "Epoch = 34, ErrG = 3.2736902236938477, ErrA = -14.609235127766928, ErrD = -15.713889439900717\n",
      "Epoch = 36, ErrG = 5.752946853637695, ErrA = -15.246897379557291, ErrD = -16.766225179036457\n",
      "Epoch = 38, ErrG = 1.6820240020751953, ErrA = -16.648352305094402, ErrD = -17.338180541992188\n",
      "Epoch = 40, ErrG = 10.724223136901855, ErrA = -17.55199940999349, ErrD = -20.6812006632487\n",
      "Epoch = 42, ErrG = 10.82100772857666, ErrA = -17.30973943074544, ErrD = -19.197577158610027\n",
      "Epoch = 44, ErrG = 6.142007827758789, ErrA = -19.585198720296223, ErrD = -21.98925272623698\n",
      "Epoch = 46, ErrG = 4.614898681640625, ErrA = -20.6905943552653, ErrD = -21.187957763671875\n",
      "Epoch = 48, ErrG = 14.935154914855957, ErrA = -20.511363983154297, ErrD = -22.849955240885418\n",
      "Epoch = 50, ErrG = 8.494245529174805, ErrA = -23.74848810831706, ErrD = -31.115222930908203\n",
      "Epoch = 52, ErrG = 9.508050918579102, ErrA = -24.220165252685547, ErrD = -25.16811243693034\n",
      "Epoch = 54, ErrG = 9.283670425415039, ErrA = -26.20640691121419, ErrD = -32.25322977701823\n",
      "Epoch = 56, ErrG = 13.134300231933594, ErrA = -23.950950622558594, ErrD = -31.37301762898763\n",
      "Epoch = 58, ErrG = 16.310850143432617, ErrA = -27.30752182006836, ErrD = -35.64556376139323\n",
      "Epoch = 60, ErrG = 9.128986358642578, ErrA = -25.83734639485677, ErrD = -38.91717656453451\n",
      "Epoch = 62, ErrG = 8.424102783203125, ErrA = -29.642303466796875, ErrD = -37.35610580444336\n",
      "Epoch = 64, ErrG = 25.559022903442383, ErrA = -31.677706400553387, ErrD = -32.23673756917318\n",
      "Epoch = 66, ErrG = 22.559337615966797, ErrA = -30.57117462158203, ErrD = -36.400753021240234\n",
      "Epoch = 68, ErrG = 15.052131652832031, ErrA = -35.96826934814453, ErrD = -46.180502573649086\n",
      "Epoch = 70, ErrG = -0.43895721435546875, ErrA = -32.98573303222656, ErrD = -43.57259877522787\n"
     ]
    }
   ],
   "source": [
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0\n",
    "margin = 0\n",
    "n_g = 1\n",
    "n_d = 2\n",
    "clip = 0.1\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "label = th.zeros((batchSize, 1), requires_grad=False).to(device)\n",
    "optimG = optim.RMSprop(netg.parameters(), lr=glr/2)\n",
    "optimD = optim.RMSprop(netd.parameters(), lr=dlr/3)\n",
    "optimA = optim.RMSprop(neta.parameters(), lr=dlr/3)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={},clip={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp, clip))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "\n",
    "        # update D\n",
    "        if i % n_g==0:\n",
    "            lossD = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "    #         label.fill_(real_label)\n",
    "    #         lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD_real1 = -th.mean(output)\n",
    "            lossD += lossD_real1.item()\n",
    "            lossD_real1.backward()\n",
    "\n",
    "    #         label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "    #         lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD_real2 = -th.mean(output1)\n",
    "            lossD += lossD_real2.item()\n",
    "            lossD_real2.backward()\n",
    "\n",
    "            fake = netg(img).detach()\n",
    "    #         label.fill_(fake_label)\n",
    "            output2 = netd(fake)\n",
    "    #         lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD_fake = th.mean(output2)\n",
    "            lossD += lossD_fake.item()\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "            for p in netd.parameters():\n",
    "                p.data.clamp_(-clip, clip)\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake = netg(img).detach()\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "    #         label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "    #         lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA_real1 = -th.mean(output1)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_real1.backward()\n",
    "\n",
    "    #         label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "    #         lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA_real2 = th.mean(output)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_real2.backward()\n",
    "\n",
    "    #         label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "    #         lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA_fake = th.mean(output)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_fake.backward()\n",
    "\n",
    "            optimA.step()\n",
    "\n",
    "            for p in neta.parameters():\n",
    "                p.data.clamp_(-clip, clip)\n",
    "\n",
    "        # update G\n",
    "        if i % n_d == 0:\n",
    "            lossG = 0\n",
    "            optimG.zero_grad()\n",
    "            fake = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "#             label.fill_(real_label)\n",
    "#             lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossGD = -th.mean(output)\n",
    "            lossG += lossGD.item()\n",
    "            lossGD.backward(retain_graph=True)\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "#             label.fill_(real_label)\n",
    "#             lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossGA = -th.mean(output)\n",
    "            lossG += lossGA.item()\n",
    "            lossGA.backward()\n",
    "\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    \n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-02T17:54:50.674179Z",
     "start_time": "2019-06-02T12:25:08.423036Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad, Variable\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00002\n",
    "dlr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 10\n",
    "beta1 = 0\n",
    "beta2 = 0.999\n",
    "margin = 0\n",
    "n_g = 0\n",
    "n_d = 5\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr, betas=(beta1, beta2))\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "# optimG = optim.RMSprop(netg.parameters(), lr=lr)\n",
    "# optimD = optim.RMSprop(netd.parameters(), lr=lr)\n",
    "# optimA = optim.RMSprop(neta.parameters(), lr=lr)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "#     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    alpha = th.rand((batchSize, 1, 1, 1)).to(device).to(th.float32)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "#     fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = grad(outputs=d_interpolates, \n",
    "                     inputs=interpolates, \n",
    "                     grad_outputs=th.ones([real_samples.shape[0],1]).to(device).requires_grad_(False),\n",
    "#                      grad_outputs = fake,\n",
    "                     create_graph=True, \n",
    "                     retain_graph=True, \n",
    "                     only_inputs=True)[0]\n",
    "#     fake = Variable((real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "#     # Get gradient w.r.t. interpolates\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=d_interpolates,\n",
    "#         inputs=interpolates,\n",
    "#         grad_outputs=fake,\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True,\n",
    "#     )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        \n",
    "        # update D\n",
    "        if i % n_g==0:\n",
    "            lossD = 0\n",
    "            lossD_ = 0\n",
    "            optimD.zero_grad()\n",
    "            output = netd(ass_label)\n",
    "    #         label.fill_(real_label)\n",
    "    #         lossD_real1 = F.binary_cross_entropy(output, label)\n",
    "            lossD_real1 = -th.mean(output)\n",
    "            lossD_ += lossD_real1\n",
    "            lossD += lossD_real1.item()\n",
    "    #         lossD_real1.backward()\n",
    "\n",
    "    #         label.fill_(real_label)\n",
    "            output1 = netd(noass_label)\n",
    "    #         lossD_real2 = F.binary_cross_entropy(output1, label)\n",
    "            lossD_real2 = -th.mean(output1)\n",
    "            lossD_ += lossD_real2\n",
    "            lossD += lossD_real2.item()\n",
    "    #         lossD_real2.backward()\n",
    "\n",
    "            fake = netg(img).detach()\n",
    "    #         label.fill_(fake_label)\n",
    "            output2 = netd(fake)\n",
    "    #         lossD_fake = F.binary_cross_entropy(output2, label)\n",
    "            lossD_fake = th.mean(output2)\n",
    "            lossD_ += lossD_fake\n",
    "            lossD += lossD_fake.item()\n",
    "            gradient_penalty = compute_gradient_penalty(netd, ass_label.data, fake.data)\n",
    "    #         lossD_fake.backward()\n",
    "            lossD_ = lossD_/3 + lambda_gp * gradient_penalty\n",
    "            lossD_.backward()\n",
    "\n",
    "            optimD.step()\n",
    "\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            lossA_ = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            fake = netg(img).detach()\n",
    "            faked = th.cat((img, fake), 1)\n",
    "\n",
    "    #         label.fill_(real_label)\n",
    "            output1 = neta(assd)\n",
    "    #         lossA_real1 = F.binary_cross_entropy(output1, label)\n",
    "            lossA_real1 = -th.mean(output1)\n",
    "            lossA += lossA_real1.item()\n",
    "            lossA_ += lossA_real1\n",
    "    #         lossA_real1.backward()\n",
    "\n",
    "    #         label.fill_(fake_label)\n",
    "            output = neta(noassd)\n",
    "    #         lossA_real2 = F.binary_cross_entropy(output, label)\n",
    "            lossA_real2 = th.mean(output)\n",
    "            lossA += lossA_real2.item()\n",
    "            lossA_ += lossA_real2\n",
    "    #         lossA_real2.backward()\n",
    "\n",
    "    #         label.fill_(fake_label)\n",
    "            output = neta(faked)\n",
    "    #         lossA_fake = F.binary_cross_entropy(output, label)\n",
    "            lossA_fake = th.mean(output)\n",
    "            lossA += lossA_fake.item()\n",
    "            lossA_ += lossA_fake\n",
    "    #         lossA_fake.backward()\n",
    "            gradient_penalty = compute_gradient_penalty(neta, assd.data, faked.data)\n",
    "            lossA_ = lossA_/3 + lambda_gp * gradient_penalty\n",
    "\n",
    "            lossA_.backward()\n",
    "            optimA.step()\n",
    "    \n",
    "            \n",
    "        # update G\n",
    "        if i % n_d == 0:\n",
    "            lossG = 0\n",
    "            lossG_ = 0\n",
    "            optimG.zero_grad()\n",
    "            fake = netg(img)\n",
    "            output = netd(fake)\n",
    "\n",
    "#             label.fill_(real_label)\n",
    "#             lossGD = F.binary_cross_entropy(output, label)\n",
    "            lossGD = -th.mean(output)\n",
    "            lossG += lossGD.item()\n",
    "            lossG_ += lossGD\n",
    "#             lossGD.backward(retain_graph=True)\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            output = neta(faked)\n",
    "#             label.fill_(real_label)\n",
    "#             lossGA = F.binary_cross_entropy(output, label)\n",
    "            lossGA = -th.mean(output)\n",
    "            lossG += lossGA.item()\n",
    "            lossG_ += lossGA\n",
    "#             lossGA.backward()\n",
    "            lossG_ = lossG_/2\n",
    "            lossG_.backward(retain_graph=True)\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))           \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA GaitGAN (hing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-13T01:45:03.277Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "n_con= 10 ,n_ang= 11\n",
      "target =  090\n",
      "write parameter log...\n",
      "Training starts\n",
      "Epoch = 2, ErrG = 1.6915537118911743, ErrA = 0.6006716887156168, ErrD = 0.08215883374214172, Gattn=0.004218496382236481, Dattn=-0.0020070704631507397, Aattn=0.0005024831043556333\n",
      "Epoch = 4, ErrG = 1.1629109978675842, ErrA = 0.38556013504664105, ErrD = 0.006895663837591807, Gattn=0.007961944676935673, Dattn=-0.0019792693201452494, Aattn=0.0009443126618862152\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad, Variable\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:0\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "\n",
    "epoches = 700\n",
    "glr = 0.00001\n",
    "dlr = 0.00004\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0.9\n",
    "margin = 0\n",
    "n_g = 2\n",
    "n_d = 1\n",
    "\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr, betas=(beta1, beta2))\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "# optimG = optim.RMSprop(netg.parameters(), lr=lr)\n",
    "# optimD = optim.RMSprop(netd.parameters(), lr=lr)\n",
    "# optimA = optim.RMSprop(neta.parameters(), lr=lr)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "#     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    alpha = th.rand((batchSize, 1, 1, 1)).to(device).to(th.float32)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "#     fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = grad(outputs=d_interpolates, \n",
    "                     inputs=interpolates, \n",
    "                     grad_outputs=th.ones([real_samples.shape[0],1]).to(device).requires_grad_(False),\n",
    "#                      grad_outputs = fake,\n",
    "                     create_graph=True, \n",
    "                     retain_graph=True, \n",
    "                     only_inputs=True)[0]\n",
    "#     fake = Variable((real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "#     # Get gradient w.r.t. interpolates\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=d_interpolates,\n",
    "#         inputs=interpolates,\n",
    "#         grad_outputs=fake,\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True,\n",
    "#     )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "\n",
    "        if i % n_g ==0:\n",
    "            # update D\n",
    "            lossD = 0\n",
    "            lossD_ = 0\n",
    "            optimD.zero_grad()\n",
    "            d_out_assreal,dr1 = netd(ass_label)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_assreal\n",
    "            lossD += d_loss_assreal.item()\n",
    "\n",
    "            d_out_noassreal,dr2 = netd(noass_label)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 - d_out_noassreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_noassreal\n",
    "            lossD += d_loss_noassreal.item()\n",
    "\n",
    "            fake, gf1 = netg(img)\n",
    "            d_out_fake, df1 = netd(fake.detach())  #需要 detach 因為不希望更新fake的參數\n",
    "            d_loss_fake = nn.ReLU()(1.0 + d_out_fake).mean()\n",
    "\n",
    "            lossD_ += d_loss_fake\n",
    "            lossD += d_loss_fake.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(netd, ass_label.data, fake.data)\n",
    "            lossD_ = lossD_/3\n",
    "            lossD_.backward()\n",
    "            optimD.step()\n",
    "\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            lossA_ = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            faked, gf1 = netg(img)\n",
    "            faked = th.cat((img, faked.detach()), 1)  #需要 detach 因為不希望更新fake的參數\n",
    "\n",
    "            d_out_assreal,dr1 = neta(assd)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "            lossA += d_loss_assreal.item()\n",
    "            lossA_ += d_loss_assreal\n",
    "\n",
    "            d_out_noassreal,dr2 = neta(noassd)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 + d_out_noassreal).mean()\n",
    "\n",
    "            lossA_ += d_loss_noassreal\n",
    "            lossA += d_loss_noassreal.item()\n",
    "\n",
    "            d_out_faked, df3 = neta(faked)\n",
    "            d_loss_faked = nn.ReLU()(1.0 + d_out_faked).mean()\n",
    "\n",
    "            lossA_ += d_loss_faked\n",
    "            lossA += d_loss_faked.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(neta, assd.data, faked.data)\n",
    "            lossA_ = lossA_/3\n",
    "            lossA_.backward()\n",
    "            optimA.step()\n",
    "\n",
    "            \n",
    "        # update G\n",
    "        if i % n_d == 0:\n",
    "            lossG = 0\n",
    "            lossG_ = 0\n",
    "            optimG.zero_grad()\n",
    "            fake,_= netg(img)\n",
    "            g_out_fake,_ = netd(fake)\n",
    "            g_loss_fake = - g_out_fake.mean()\n",
    "\n",
    "            lossG += g_loss_fake.item()\n",
    "            lossG_ += g_loss_fake\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            g_out_faked,_ = neta(faked)\n",
    "            g_loss_faked = - g_out_faked.mean()\n",
    "            lossG += g_loss_faked.item()\n",
    "            lossG_ += g_loss_faked\n",
    "\n",
    "            lossG_ = lossG_/2\n",
    "            lossG_.backward(retain_graph=True) ##其實這裡也不需要retain graph 因為他只 backward 一次；\n",
    "            #如果爾後有要再backward，第一次就需要retain graph\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}, Gattn={}, Dattn={}, Aattn={}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3, netg.attn.gamma.item(), netd.attn.gamma.item(), neta.attn.gamma.item()\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))           \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA GaitGAN (WGAN-GP) 刪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T00:07:22.217162Z",
     "start_time": "2019-06-05T16:58:50.881140Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad, Variable\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00001\n",
    "dlr = 0.00004\n",
    "# lr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "n_critic = 0\n",
    "target = '090'\n",
    "lambda_gp = 10\n",
    "beta1 = 0\n",
    "beta2 = 0.9\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr, betas=(beta1, beta2))\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "# optimG = optim.RMSprop(netg.parameters(), lr=lr)\n",
    "# optimD = optim.RMSprop(netd.parameters(), lr=lr)\n",
    "# optimA = optim.RMSprop(neta.parameters(), lr=lr)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_critic = {}, target={},lambda_gp={} \\n'.format(\n",
    "            epoches, dlr, glr, batchSize, beta1, beta2, n_critic, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "#     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    alpha = th.rand((batchSize, 1, 1, 1)).to(device).to(th.float32)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "#     fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = grad(outputs=d_interpolates, \n",
    "                     inputs=interpolates, \n",
    "                     grad_outputs=th.ones([real_samples.shape[0],1]).to(device).requires_grad_(False),\n",
    "#                      grad_outputs = fake,\n",
    "                     create_graph=True, \n",
    "                     retain_graph=True, \n",
    "                     only_inputs=True)[0]\n",
    "#     fake = Variable((real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "#     # Get gradient w.r.t. interpolates\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=d_interpolates,\n",
    "#         inputs=interpolates,\n",
    "#         grad_outputs=fake,\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True,\n",
    "#     )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "    for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "    # while epoch < 30000:\n",
    "    #     ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        \n",
    "        \n",
    "        # update D\n",
    "        lossD = 0\n",
    "        lossD_ = 0\n",
    "        optimD.zero_grad()\n",
    "        d_out_assreal,dr1 = netd(ass_label)\n",
    "        d_loss_assreal = -th.mean(d_out_assreal)\n",
    "\n",
    "        lossD_ += d_loss_assreal\n",
    "        lossD += d_loss_assreal.item()\n",
    "\n",
    "        d_out_noassreal,dr2 = netd(noass_label)\n",
    "        d_loss_noassreal = -th.mean(d_out_noassreal)\n",
    "\n",
    "        lossD_ += d_loss_noassreal\n",
    "        lossD += d_loss_noassreal.item()\n",
    "\n",
    "        fake, gf1 = netg(img)\n",
    "        d_out_fake, df1 = netd(fake.detach())  #需要 detach 因為不希望更新fake的參數\n",
    "        d_loss_fake = th.mean(d_out_fake)\n",
    "\n",
    "        lossD_ += d_loss_fake\n",
    "        lossD += d_loss_fake.item()\n",
    "        gradient_penalty = compute_gradient_penalty(netd, ass_label.data, fake.data)\n",
    "        lossD_ = lossD_/3 + lambda_gp * gradient_penalty\n",
    "#         lossD_ = lossD_/3\n",
    "        lossD_.backward()\n",
    "        optimD.step()\n",
    "        \n",
    "            \n",
    "        # update A\n",
    "        lossA = 0\n",
    "        lossA_ = 0\n",
    "        optimA.zero_grad()\n",
    "        assd = th.cat((img, ass_label), 1)\n",
    "        noassd = th.cat((img, noass_label), 1)\n",
    "        faked, gf1 = netg(img)\n",
    "        faked = th.cat((img, faked.detach()), 1)  #需要 detach 因為不希望更新fake的參數\n",
    "\n",
    "        d_out_assreal,dr1 = neta(assd)\n",
    "        d_loss_assreal = -th.mean(d_out_assreal)\n",
    "        lossA += d_loss_assreal.item()\n",
    "        lossA_ += d_loss_assreal\n",
    "\n",
    "        d_out_noassreal,dr2 = neta(noassd)\n",
    "        d_loss_noassreal = th.mean(d_out_noassreal)\n",
    "\n",
    "        lossA_ += d_loss_noassreal\n",
    "        lossA += d_loss_noassreal.item()\n",
    "        \n",
    "        d_out_faked, df3 = neta(faked)\n",
    "        d_loss_faked = th.mean(d_out_faked)\n",
    "\n",
    "        lossA_ += d_loss_faked\n",
    "        lossA += d_loss_faked.item()\n",
    "        gradient_penalty = compute_gradient_penalty(neta, assd.data, faked.data)\n",
    "        lossA_ = lossA_/3 + lambda_gp * gradient_penalty\n",
    "#         lossA_ = lossA_/3\n",
    "        lossA_.backward()\n",
    "        optimA.step()\n",
    "\n",
    "            \n",
    "        # update G\n",
    "#         if i % n_critic == 0:\n",
    "        lossG = 0\n",
    "        lossG_ = 0\n",
    "        optimG.zero_grad()\n",
    "        fake,_= netg(img)\n",
    "        g_out_fake,_ = netd(fake)\n",
    "        g_loss_fake = - g_out_fake.mean()\n",
    "\n",
    "        lossG += g_loss_fake.item()\n",
    "        lossG_ += g_loss_fake\n",
    "\n",
    "        faked = th.cat((img, fake), 1)\n",
    "        g_out_faked,_ = neta(faked)\n",
    "        g_loss_faked = - g_out_faked.mean()\n",
    "        lossG += g_loss_faked.item()\n",
    "        lossG_ += g_loss_faked\n",
    "        \n",
    "        lossG_ = lossG_/2\n",
    "        lossG_.backward(retain_graph=True) ##其實這裡也不需要retain graph 因為他只 backward 一次；\n",
    "        #如果爾後有要再backward，第一次就需要retain graph\n",
    "        optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/2, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {}, Gattn={}, Dattn={}, Aattn={}'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3, netg.attn.gamma.item(), netd.attn.gamma.item(), neta.attn.gamma.item()\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/2)<low_loss:  \n",
    "        low_loss = lossG/2\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/2, lossA/3, lossD/3\n",
    "        ))           \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA GaitGAN and triplet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T11:04:01.107626Z",
     "start_time": "2019-07-06T04:16:43.132497Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad, Variable\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "\n",
    "epoches = 700\n",
    "glr = 0.00001\n",
    "dlr = 0.00004\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0.9\n",
    "margin = 10\n",
    "n_g = 2\n",
    "n_d = 0\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr, betas=(beta1, beta2))\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "# optimG = optim.RMSprop(netg.parameters(), lr=lr)\n",
    "# optimD = optim.RMSprop(netd.parameters(), lr=lr)\n",
    "# optimA = optim.RMSprop(neta.parameters(), lr=lr)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "#     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    alpha = th.rand((batchSize, 1, 1, 1)).to(device).to(th.float32)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "#     fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = grad(outputs=d_interpolates, \n",
    "                     inputs=interpolates, \n",
    "                     grad_outputs=th.ones([real_samples.shape[0],1]).to(device).requires_grad_(False),\n",
    "#                      grad_outputs = fake,\n",
    "                     create_graph=True, \n",
    "                     retain_graph=True, \n",
    "                     only_inputs=True)[0]\n",
    "#     fake = Variable((real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "#     # Get gradient w.r.t. interpolates\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=d_interpolates,\n",
    "#         inputs=interpolates,\n",
    "#         grad_outputs=fake,\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True,\n",
    "#     )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "#     for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "    for i, (ass_label, noass_label, noass_img, img, ass_img) in enumerate(train_loader):\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "    # while epoch < 30000:\n",
    "    #     ass_label, noass_label, img = dataset.getbatch(batchSize)\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        noass_img = noass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        ass_img = ass_img.to(device).to(th.float32)\n",
    "\n",
    "        if i % n_g ==0:\n",
    "            # update D\n",
    "            lossD = 0\n",
    "            lossD_ = 0\n",
    "            optimD.zero_grad()\n",
    "            d_out_assreal,dr1 = netd(ass_label)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_assreal\n",
    "            lossD += d_loss_assreal.item()\n",
    "\n",
    "            d_out_noassreal,dr2 = netd(noass_label)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 - d_out_noassreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_noassreal\n",
    "            lossD += d_loss_noassreal.item()\n",
    "\n",
    "            fake, code = netg(img)\n",
    "            d_out_fake, df1 = netd(fake.detach())\n",
    "            d_loss_fake = nn.ReLU()(1.0 + d_out_fake).mean()\n",
    "\n",
    "            lossD_ += d_loss_fake\n",
    "            lossD += d_loss_fake.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(netd, ass_label.data, fake.data)\n",
    "            lossD_ = lossD_/3\n",
    "            lossD_.backward()\n",
    "            optimD.step()\n",
    "\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            lossA_ = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            faked, code = netg(img)\n",
    "            faked = th.cat((img, faked.detach()), 1)\n",
    "\n",
    "            d_out_assreal,dr1 = neta(assd)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "            lossA += d_loss_assreal.item()\n",
    "            lossA_ += d_loss_assreal\n",
    "\n",
    "            d_out_noassreal,dr2 = neta(noassd)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 + d_out_noassreal).mean()\n",
    "\n",
    "            lossA_ += d_loss_noassreal\n",
    "            lossA += d_loss_noassreal.item()\n",
    "\n",
    "            d_out_faked, df3 = neta(faked)\n",
    "            d_loss_faked = nn.ReLU()(1.0 + d_out_faked).mean()\n",
    "\n",
    "            lossA_ += d_loss_faked\n",
    "            lossA += d_loss_faked.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(neta, assd.data, faked.data)\n",
    "            lossA_ = lossA_/3\n",
    "            lossA_.backward()\n",
    "            optimA.step()\n",
    "\n",
    "            \n",
    "        # update G\n",
    "        if i % n_d == 0:\n",
    "            lossG = 0\n",
    "            lossG_ = 0\n",
    "            optimG.zero_grad()\n",
    "            fake, A= netg(img)\n",
    "            g_out_fake,_ = netd(fake)\n",
    "            g_loss_fake = - g_out_fake.mean()\n",
    "\n",
    "            lossG += g_loss_fake.item()\n",
    "            lossG_ += g_loss_fake\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            g_out_faked,_ = neta(faked)\n",
    "            g_loss_faked = - g_out_faked.mean()\n",
    "            lossG += g_loss_faked.item()\n",
    "            lossG_ += g_loss_faked\n",
    "\n",
    "            # constrain on generator\n",
    "            fake_ass, P = netg(ass_img)\n",
    "            fake_noass, N = netg(noass_img)\n",
    "            lossTriplet = F.triplet_margin_loss(A, P, N, margin = margin)\n",
    "            lossG_ += lossTriplet\n",
    "            lossG += lossTriplet.item()\n",
    "    #         lossTriplet.backward()\n",
    "\n",
    "            lossG_ = lossG_/3\n",
    "            lossG_.backward(retain_graph=True)\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/3, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/3, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/3, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrTri = {}, ErrA = {}, ErrD = {}, Gattn={}, Dattn={}, Aattn={}'.format(\n",
    "            epoch, lossG/3,lossTriplet.item(), lossA/3, lossD/3, netg.attn.gamma.item(), netd.attn.gamma.item(), neta.attn.gamma.item()\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/3)<low_loss:  \n",
    "        low_loss = lossG/3\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/3, lossA/3, lossD/3\n",
    "        ))           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA GaitGAN and triplet (onlineloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T00:29:55.922173Z",
     "start_time": "2019-07-06T16:40:51.825605Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.autograd import grad, Variable\n",
    "\n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00001\n",
    "dlr = 0.00004\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0.9\n",
    "margin = 10\n",
    "n_g = 2\n",
    "n_d = 0\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr, betas=(beta1, beta2))\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "# optimG = optim.RMSprop(netg.parameters(), lr=lr)\n",
    "# optimD = optim.RMSprop(netd.parameters(), lr=lr)\n",
    "# optimA = optim.RMSprop(neta.parameters(), lr=lr)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, batchsize = {}, beta1={}, beta2={}, n_d = {}, n_g={} target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, batchSize, beta1, beta2, n_d, n_g, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "#     alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    alpha = th.rand((batchSize, 1, 1, 1)).to(device).to(th.float32)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "#     fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = grad(outputs=d_interpolates, \n",
    "                     inputs=interpolates, \n",
    "                     grad_outputs=th.ones([real_samples.shape[0],1]).to(device).requires_grad_(False),\n",
    "#                      grad_outputs = fake,\n",
    "                     create_graph=True, \n",
    "                     retain_graph=True, \n",
    "                     only_inputs=True)[0]\n",
    "#     fake = Variable((real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "#     # Get gradient w.r.t. interpolates\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=d_interpolates,\n",
    "#         inputs=interpolates,\n",
    "#         grad_outputs=fake,\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True,\n",
    "#     )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "#     for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "    for i, (ass_label, noass_label, noass_img, img, ass_img, label_neg, label_anc, label_pos) in enumerate(train_loader):\n",
    "        \n",
    "        com_img = th.cat((noass_img, img, ass_img), 0)\n",
    "        com_label = th.cat(( label_neg, label_anc, label_pos), 0)\n",
    "        com_img = com_img.to(device).to(th.float32)\n",
    "        com_label = com_label.to(device).to(th.float32)\n",
    "#         print(\"shape\",ass_label.shape,noass_label.shape,img.shape)\n",
    "        \n",
    "\n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        noass_img = noass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        ass_img = ass_img.to(device).to(th.float32)\n",
    "\n",
    "        if i % n_g ==0:\n",
    "            # update D\n",
    "            lossD = 0\n",
    "            lossD_ = 0\n",
    "            optimD.zero_grad()\n",
    "            d_out_assreal,dr1 = netd(ass_label)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_assreal\n",
    "            lossD += d_loss_assreal.item()\n",
    "\n",
    "            d_out_noassreal,dr2 = netd(noass_label)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 - d_out_noassreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_noassreal\n",
    "            lossD += d_loss_noassreal.item()\n",
    "\n",
    "            fake, code = netg(img)\n",
    "            d_out_fake, df1 = netd(fake.detach())\n",
    "            d_loss_fake = nn.ReLU()(1.0 + d_out_fake).mean()\n",
    "\n",
    "            lossD_ += d_loss_fake\n",
    "            lossD += d_loss_fake.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(netd, ass_label.data, fake.data)\n",
    "            lossD_ = lossD_/3\n",
    "            lossD_.backward()\n",
    "            optimD.step()\n",
    "\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            lossA_ = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            faked, code = netg(img)\n",
    "            faked = th.cat((img, faked.detach()), 1)\n",
    "\n",
    "            d_out_assreal,dr1 = neta(assd)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "            lossA += d_loss_assreal.item()\n",
    "            lossA_ += d_loss_assreal\n",
    "\n",
    "            d_out_noassreal,dr2 = neta(noassd)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 + d_out_noassreal).mean()\n",
    "\n",
    "            lossA_ += d_loss_noassreal\n",
    "            lossA += d_loss_noassreal.item()\n",
    "\n",
    "            d_out_faked, df3 = neta(faked)\n",
    "            d_loss_faked = nn.ReLU()(1.0 + d_out_faked).mean()\n",
    "\n",
    "            lossA_ += d_loss_faked\n",
    "            lossA += d_loss_faked.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(neta, assd.data, faked.data)\n",
    "            lossA_ = lossA_/3\n",
    "            lossA_.backward()\n",
    "            optimA.step()\n",
    "\n",
    "\n",
    "            \n",
    "        # update G\n",
    "        if i % n_d == 0:\n",
    "            lossG = 0\n",
    "            lossG_ = 0\n",
    "            optimG.zero_grad()\n",
    "            fake, A= netg(img)\n",
    "            g_out_fake,_ = netd(fake)\n",
    "            g_loss_fake = - g_out_fake.mean()\n",
    "\n",
    "            lossG += g_loss_fake.item()\n",
    "            lossG_ += g_loss_fake\n",
    "\n",
    "            faked = th.cat((img, fake), 1)\n",
    "            g_out_faked,_ = neta(faked)\n",
    "            g_loss_faked = - g_out_faked.mean()\n",
    "            lossG += g_loss_faked.item()\n",
    "            lossG_ += g_loss_faked\n",
    "\n",
    "    #         # constrain on encoder\n",
    "    #         fake_ass, P = netg(ass_img)\n",
    "    #         fake_noass, N = netg(noass_img)\n",
    "    #         lossTriplet = F.triplet_margin_loss(A, P, N, margin = margin)\n",
    "    #         lossG_ += lossTriplet\n",
    "    #         lossG += lossTriplet.item()\n",
    "    # #         lossTriplet.backward()\n",
    "\n",
    "            ## new onlinetripletloss\n",
    "            __, com = netg(com_img)\n",
    "            loss_fn = OnlineTripletLoss(margin, SemihardNegativeTripletSelector(margin))\n",
    "            lossTriplet,len_triplet = loss_fn(com, com_label)\n",
    "            lossG += lossTriplet.item()\n",
    "            lossG_ += lossTriplet\n",
    "    #         lossTriplet.backward()\n",
    "\n",
    "\n",
    "            lossG_ = lossG_/3\n",
    "            lossG_.backward(retain_graph=True)\n",
    "            optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/3, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/3, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/3, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrTri = {}, ErrA = {}, ErrD = {}, Gattn={}, Dattn={}, Aattn={}'.format(\n",
    "            epoch, lossG/3,lossTriplet.item(), lossA/3, lossD/3, netg.attn.gamma.item(), netd.attn.gamma.item(), neta.attn.gamma.item()\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/3)<low_loss:  \n",
    "        low_loss = lossG/3\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/3, lossA/3, lossD/3\n",
    "        ))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36_ting_cv]",
   "language": "python",
   "name": "conda-env-py36_ting_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
